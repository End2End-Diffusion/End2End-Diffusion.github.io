<!doctype html>
<html lang="en">
    <head>
        <title>REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers</title>
        <link rel="icon" type="image/png" href="../static/img/kitty.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://end2end-diffusion.github.io/repa-e-t2i" />
        <meta property="og:title" content="REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers" />
        <meta property="og:description" content="End-to-end tunable VAEs for improved text-to-image generation with diffusion transformers. Better performance across metrics with maintained ImageNet generalization." />

        <!-- Twitter -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers" />
        <meta name="twitter:description" content="End-to-end tunable VAEs for improved text-to-image generation with diffusion transformers." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script defer="" src="./static/js/hider.js"></script>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

        <link rel="stylesheet" href="../static/css/paper-layout.css">
        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/custom.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- medium zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <!-- Prism.js for syntax highlighting -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
        <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>

        <!-- Tab switching for Quickstart -->
        <script defer src="./static/js/tabs.js"></script>
    </head>
    <body>
        <!-- Sticky Table of Contents -->
        <nav class="toc-container">
            <h4>Contents</h4>
            <ul>
                <li><a href="#quickstart">Quickstart</a></li>
                <li><a href="#training">Training Recipe</a></li>
                <li><a href="#quantitative">T2I Results</a></li>
                <li><a href="#latent-analysis">Latent Analysis</a></li>
                <li><a href="#imagenet">ImageNet</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
            </ul>
        </nav>

        <nav style="background-color: #fff; border-bottom: 1px solid #e5e5e5; padding: 12px 0;">
            <div style="max-width: 1200px; margin: 0 auto; padding: 0 20px; display: flex; justify-content: space-between; align-items: center;">
                <a href="../index.html" style="color: #1a1a1a; text-decoration: none; font-size: 15px; font-weight: 500;">
                    <i class="fas fa-arrow-left" style="margin-right: 8px; opacity: 0.5;"></i>
                    End2End Diffusion
                </a>
                <div style="display: flex; gap: 20px;">
                    <a href="../repa-e/" style="color: #666; text-decoration: none; font-size: 14px;">REPA-E</a>
                    <a href="../irepa/" style="color: #666; text-decoration: none; font-size: 14px;">iREPA</a>
                </div>
            </div>
        </nav>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <!-- Title section - full width, centered -->
                <div class="header-title">
                    <h1 style="margin-top: 0px">REPA-E <em>for</em> T2I</h1>
                    <h2>Family of end-to-end tuned VAEs for <em>supercharging</em><br>T2I diffusion transformers</h2>
                    <blockquote style="border-left: 3px solid #2563eb; padding-left: 1em; margin: 1em auto; max-width: 800px; font-style: italic; color: #444;">
                        We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                        <strong style="color: #2563eb;">End-to-end VAEs show superior performance over their original counterparts</strong> across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) without need for any additional representation alignment losses.
                    </blockquote>

                    <hr style="border: none; border-top: 1px solid #e5e5e5; margin: 1.5em auto; max-width: 600px;">
                </div>

                <!-- Two-column section: key points left, hero image right -->
                <div class="header-columns">
                    <div class="header-content">
                        <!-- Key Points Icon Container -->
                        <div class="icon-container">
                            <div class="icon-item">
                                <img src="./static/img/icons/accuracy.svg" alt="Performance Icon">
                                <div><strong>Better T2I Performance</strong>: Consistent improvements over baseline VAEs across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) with end-to-end tuned VAEs.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/spatial.svg" alt="Quality Icon">
                                <div><strong>Family of End-to-End Tuned VAEs</strong>: We release a family of end-to-end tuned VAEs across different families (FLUX-VAE, SD3.5-VAE, Qwen-Image-VAE) for supercharging text-to-image generation training.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/irepa.svg" alt="Generalization Icon">
                                <div><strong>Better Latent Space Structure</strong>: End-to-end tuned VAEs show better latent space structure compared to their original counterparts which are mostly optimized for reconstruction.</div>
                            </div>
                        </div>

                        <div class="button-container">
                            <a href="https://arxiv.org/abs/2504.10483" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                REPA-E Paper
                            </a>
                            <a href="https://github.com/End2End-Diffusion/REPA-E" class="button" target="_blank">
                                <span class="icon is-small">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                            </a>
                            <a href="https://huggingface.co/REPA-E" class="button" target="_blank">
                                <span class="icon is-small">
                                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                                </span>
                                <span>Models</span>
                            </a>
                        </div>
                    </div>
                    <div class="header-image">
                        <!-- Hero image -->
                        <img draggable="false" src="repa-e-t2i-v7.png" alt="End-to-End Training: VAE + Diffusion Transformer" class="teaser-image">
                    </div>
                </div>
            </div>
        </div>
    <d-article>
        <!-- <div class="byline">
            <div class="byline-container">
                <p align="center">
                    <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian&nbsp;Leng</a><sup>1,2*</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://1jsingh.github.io/" target="_blank">Jaskirat&nbsp;Singh</a><sup>1</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://rynmurdock.github.io/" target="_blank">Ryan&nbsp;Murdock</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://www.ethansmith2000.com/" target="_blank">Ethan&nbsp;Smith</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://xiaoyang-rebecca.github.io/cv/" target="_blank">Rebecca&nbsp;Li</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://www.sainingxie.com/" target="_blank">Saining&nbsp;Xie</a><sup>3</sup>&ensp; <b>&middot;</b> &ensp;
                    <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang&nbsp;Zheng</a><sup>1</sup>&ensp;
                </p>
                <p align="center">
                    <sup>1</sup> Australian National University &emsp; <sup>2</sup>Canva &emsp; <sup>3</sup>New York University &emsp; <br>
                    <sub><sup>*</sup>Done during internship at Canva &emsp;</sub>
                </p>
            </div>
        </div> -->

        <!-- Collaboration Heading -->
        <div style="text-align: center; margin: 40px 0 40px 0;">
            <!-- Logo and collaboration line (largest) -->
            <div style="display: flex;
                        align-items: center;
                        justify-content: center;
                        gap: 20px;
                        margin-bottom: 15px;">
                <img src="./static/img/logos/repa-e-logo.png" alt="REPA-E"
                     style="height: 60px;">
                <h2 style="color: #6c757d;
                           font-weight: 300;
                           margin: 0;
                           font-size: 1.8em;
                           letter-spacing: 1px;
                           font-family: 'Georgia', 'Times New Roman', serif;">
                    <a href="https://github.com/End2End-Diffusion/REPA-E" target="_blank" style="color: #6c757d; text-decoration: none;">REPA-E</a> <span style="filter: grayscale(100%);">ü§ù</span> <a href="https://x.com/canva" target="_blank" style="color: #6c757d; text-decoration: none;">Canva</a>
                </h2>
                <img src="./static/img/logos/canva-logo.png" alt="Canva"
                     style="height: 60px;">
            </div>

            <!-- "presents" line (smaller) -->
            <p style="color: #6c757d;
                      font-weight: 300;
                      margin: 10px 0;
                      font-size: 1.1em;
                      letter-spacing: 0.5px;
                      font-family: 'Georgia', 'Times New Roman', serif;">
                presents
            </p>

            <!-- Subtitle line (medium) -->
            <p style="color: #6c757d;
                      font-weight: 400;
                      margin: 10px 0 0 0;
                      font-size: 1.2em;
                      letter-spacing: 0.3px;
                      line-height: 1.4;
                      white-space: nowrap;
                      font-family: 'Georgia', 'Times New Roman', serif;">
                Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers
            </p>

            <!-- Team Members -->
            <p style="color: #6c757d;
                      font-weight: 400;
                      margin: 15px 0 5px 0;
                      font-size: 0.95em;
                      line-height: 1.8;">
                <strong style="color: #555;">REPA-E Team:</strong>
                <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank" style="color: #6c757d;">Xingjian Leng<sup>*</sup></a>,
                <a href="https://1jsingh.github.io/" target="_blank" style="color: #6c757d;">Jaskirat Singh<sup>*</sup></a>,
                <a href="https://hou-yz.github.io/" target="_blank" style="color: #6c757d;">Yunzhong Hou</a>,
                <a href="https://people.csiro.au/X/Z/Zhenchang-Xing/" target="_blank" style="color: #6c757d;">Zhenchang Xing</a>,
                <a href="https://www.sainingxie.com/" target="_blank" style="color: #6c757d;">Saining Xie</a>,
                <a href="https://zheng-lab-anu.github.io/" target="_blank" style="color: #6c757d;">Liang Zheng</a>
                <br>
                <strong style="color: #555;">Canva Team:</strong>
                <a href="https://rynmurdock.github.io/" target="_blank" style="color: #6c757d;">Ryan Murdock</a>,
                <a href="https://www.ethansmith2000.com/" target="_blank" style="color: #6c757d;">Ethan Smith</a>,
                <a href="https://xiaoyang-rebecca.github.io/cv/" target="_blank" style="color: #6c757d;">Rebecca Li</a>
            </p>
        </div>

        <hr style="width: 60%; margin: 20px auto; border: none; border-top: 1px solid #d0d0d0;">

        <div class="sidenote-container">
            <p class="text">
                We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                End-to-end VAEs show superior performance across all benchmarks (COCO30k<d-cite key="coco30k"></d-cite>, DPG-Bench<d-cite key="dpgbench"></d-cite>, GenAI-Bench<d-cite key="genaibench"></d-cite>, GenEval<d-cite key="geneval"></d-cite>, MJHQ30k<d-cite key="mjhq30k"></d-cite>) without need for any additional representation alignment losses!
            </p>
            <aside class="sidenote">End-to-end tuned VAEs improve T2I generation without needing REPA's representation alignment losses during training.</aside>
        </div>
        <p class="text">
            <!-- Beyond T2I improvements, we show that end-to-end tuning enhances latent space structure and maintains strong ImageNet<d-cite key="imagenet"></d-cite> generalization. -->
        <!-- </p>

        <p class="text"> -->
            <!-- Through comprehensive evaluation across multiple scales, datasets, and resolutions, we reveal four key findings: -->
            <ol class="text">
                <li><strong><a href="#quantitative">&sect;Better T2I Performance</a></strong>: End-to-End tuned VAEs lead to better T2I generation without need for any additional representation alignment losses!</li>
                <li><strong><a href="#quantitative">&sect;End-to-End Training on ImageNet Generalizes to T2I</a></strong>: End-to-End VAEs tuned on just "ImageNet 256√ó256" <d-cite key="imagenet"></d-cite> generalize for better "T2I generation" across different resolutions (256√ó256, 512√ó512).</li>
                <li><strong><a href="#latent-analysis">&sect;Improved Latent Space Structure</a></strong>: End-to-End tuned VAEs show improved semantic spatial structure and details over traditionally used VAEs like FLUX-VAE, SD3.5-VAE, Qwen-Image-VAE etc.</li>
                <li><strong><a href="#imagenet">&sect;SOTA performance on Imagenet</a></strong>: End-to-End tuned VAEs achieve new state-of-the-art performance on ImageNet 256√ó256 <d-cite key="imagenet"></d-cite> achieving gFID 1.12 with classifer-free guidance.</li>
            </ol>
        </p>

        <!-- Jump to Sections -->
        <div class="icon-row">
            <a href="#quickstart" class="icon-link">
                <img src="./static/img/icons/quickstart-blue.svg" alt="Quickstart" class="icon">
                Quickstart
            </a>
            <a href="#training" class="icon-link">
                <img src="./static/img/icons/recipe-blue.svg" alt="Training Recipe" class="icon">
                Training Recipe
            </a>
            <a href="#quantitative" class="icon-link">
                <img src="./static/img/icons/accuracy-blue.svg" alt="Quantitative" class="icon">
                T2I Results
            </a>
            <a href="#latent-analysis" class="icon-link">
                <img src="./static/img/icons/irepa-blue.svg" alt="Latent Analysis" class="icon">
                Latent Analysis
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>

        <hr>

        <!-- Quickstart Section -->
        <div class="quickstart-section" id="quickstart">
            <h1 class="text">Quickstart</h1>

            <!-- Tab Buttons -->
            <div class="tab-container">
                <button class="tab-button" data-tab="tab-flux">E2E-FLUX-VAE</button>
                <button class="tab-button" data-tab="tab-sd35">E2E-SD3.5-VAE</button>
                <button class="tab-button" data-tab="tab-qwen">E2E-Qwen-Image-VAE</button>
                <button class="tab-button" data-tab="tab-sdvae">E2E-SD-VAE</button>
                <button class="tab-button" data-tab="tab-vavae">E2E-VA-VAE</button>
                <button class="tab-button" data-tab="tab-invae">E2E-INVAE</button>
            </div>

            <!-- FLUX-VAE Tab Content -->
            <div id="tab-flux" class="tab-content">
                <div class="install-note">
                    <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
                </div>

                <div class="quickstart-example">
                    <h3>Quick Start</h3>
                    <p>Loading the VAE is as easy as:</p>
                    <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-flux-vae").to("cuda")</code></pre>
                </div>

                <div class="quickstart-example">
                    <h3>Complete Example</h3>
                    <p>Full workflow for encoding and decoding images:</p>
                    <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-flux-vae").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
                </div>
            </div>

            <!-- SD3.5-VAE Tab Content -->
            <div id="tab-sd35" class="tab-content">
                <div class="install-note">
                    <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
                </div>

                <div class="quickstart-example">
                    <h3>Quick Start</h3>
                    <p>Loading the VAE is as easy as:</p>
                    <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sd3.5-vae").to("cuda")</code></pre>
                </div>

                <div class="quickstart-example">
                    <h3>Complete Example</h3>
                    <p>Full workflow for encoding and decoding images:</p>
                    <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sd3.5-vae").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
                </div>
            </div>

            <!-- Qwen-Image-VAE Tab Content -->
            <div id="tab-qwen" class="tab-content">
                <div class="install-note">
                    <strong>Installation:</strong> <code>pip install diffusers>=0.35.0 torch>=2.5.0</code>
                </div>

                <div class="quickstart-example">
                    <h3>Quick Start</h3>
                    <p>Loading the VAE is as easy as:</p>
                    <pre><code class="language-python">from diffusers import AutoencoderKLQwenImage

vae = AutoencoderKLQwenImage.from_pretrained("REPA-E/e2e-qwenimage-vae").to("cuda")</code></pre>
                </div>

                <div class="quickstart-example">
                    <h3>Complete Example</h3>
                    <p>Full workflow for encoding and decoding images (note the frame dimension handling):</p>
                    <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKLQwenImage
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKLQwenImage.from_pretrained("REPA-E/e2e-qwenimage-vae").to(device)

# Add frame dimension (required for QwenImage VAE)
image_ = image.unsqueeze(2)

with torch.no_grad():
    latents = vae.encode(image_).latent_dist.sample()
    reconstructed = vae.decode(latents).sample

# Remove frame dimension
latents = latents.squeeze(2)
reconstructed = reconstructed.squeeze(2)</code></pre>
                </div>
            </div>

            <!-- SD-VAE Tab Content -->
            <div id="tab-sdvae" class="tab-content">
                <div class="install-note">
                    <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
                </div>

                <div class="quickstart-example">
                    <h3>Quick Start</h3>
                    <p>Loading the VAE is as easy as:</p>
                    <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sdvae-hf").to("cuda")</code></pre>
                </div>

                <div class="quickstart-example">
                    <h3>Complete Example</h3>
                    <p>Full workflow for encoding and decoding images (512√ó512 resolution):</p>
                    <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content)).resize((512, 512))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sdvae-hf").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
                </div>
            </div>

            <!-- VA-VAE Tab Content -->
            <div id="tab-vavae" class="tab-content">
                <div class="install-note">
                    <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
                </div>

                <div class="quickstart-example">
                    <h3>Quick Start</h3>
                    <p>Loading the VAE is as easy as:</p>
                    <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-vavae-hf").to("cuda")</code></pre>
                </div>

                <div class="quickstart-example">
                    <h3>Complete Example</h3>
                    <p>Full workflow for encoding and decoding images (512√ó512 resolution):</p>
                    <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content)).resize((512, 512))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-vavae-hf").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
                </div>
            </div>

            <!-- InVAE Tab Content -->
            <div id="tab-invae" class="tab-content">
                <div class="install-note">
                    <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
                </div>

                <div class="quickstart-example">
                    <h3>Quick Start</h3>
                    <p>Loading the VAE is as easy as:</p>
                    <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-invae-hf").to("cuda")</code></pre>
                </div>

                <div class="quickstart-example">
                    <h3>Complete Example</h3>
                    <p>Full workflow for encoding and decoding images (512√ó512 resolution):</p>
                    <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content)).resize((512, 512))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-invae-hf").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
                </div>
            </div>
        </div>

        <hr>

        <!-- End-to-End VAE Training Section -->
        <div class="method-block" id="training">
            <h1 class="text">End-to-End VAE Training Recipe and T2I Setup</h1>

            <div class="sidenote-container">
                <p class="text">
                    We perform end-to-end tuning on ImageNet 256√ó256 <d-cite key="imagenet"></d-cite> to obtain end-to-end tuned encoders for popular VAE families like FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>.
                    We then compare the performance of obtained end-to-end tuned VAEs with the standard VAEs (e.g, Flux-VAE) for text-to-image (T2I) generation tasks.
                    <!-- Before moving on to text-to-image generation tasks, we first perform end-to-end tuning to VAEs.
                    We use the official REPA-E<d-cite key="leng2025repa"></d-cite> implementation with the same hyperparameter as the original paper, except for the learning rate.
                    Since we are fine-tuning already high-quality VAEs (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>), we opt for a smaller learning rate at 2√ó10<sup>-5</sup>. -->
                </p>
                <aside class="sidenote">The key insight is that VAEs optimized only for reconstruction may not produce optimal latent spaces for generation.</aside>
            </div>
            
            <p class="text">
                <strong>End-to-End VAE Tuning on ImageNet 256√ó256.</strong> We follow the same training recipe as the original REPA-E<d-cite key="leng2025repa"></d-cite> paper, for end-to-end tuning on ImageNet 256√ó256.
                We use a small learning rate of 2√ó10<sup>-5</sup>, AdamW optimizer and 80 epochs for end-to-end tuning across all VAE families (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>). We next refer the corresponding finetuned VAEs as E2E-FLUX-VAE, E2E-SD-3.5-VAE, E2E-Qwen-Image-VAE respectively.
            </p>
            <div class="config-box">
                <p class="text"><strong>End-to-End Tuning Configuration for VAE:</strong></p>
                <ul class="text">
                    <li><strong>VAE Models:</strong> FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite></li>
                    <li><strong>Dataset:</strong> ImageNet-256<d-cite key="imagenet"></d-cite></li>
                    <li><strong>Training epochs:</strong> 80 epochs</li>
                    <li><strong>Learning rate:</strong> 2√ó10<sup>-5</sup></li>
                </ul>
            </div>

            <p class="text">
                <strong>T2I Training Setup.</strong> For our diffusion backbone, we follow the setup in Fuse-DiT<d-cite key="fuse-dit"></d-cite> and adopt a variant of the DiT-3B<d-cite key="dit"></d-cite> architecture with a self-attention-based text conditioning mechanism.
                We use Gemma-2B<d-cite key="gemma"></d-cite> to encode text prompts into contextual embeddings.
                <!-- Specifically, text representations are projected to produce key and value states, which are concatenated with the keys and values derived from image latent features before the attention operation. -->
                <!-- The query states come solely from the image features, allowing the model to attend to both visual and textual information simultaneously within a unified self-attention layer. -->
                For training we use the BLIP-3o<d-cite key="blip3o"></d-cite> pretraining dataset (~28M samples) for T2I training with both original and end-to-end tuned VAEs. We perform experiments at both 256√ó256 and 512√ó512 resolutions. Unless otherwise specified, we use 25 sampling steps with a guidance scale of 6.5 for inference.
            </p>

            <div class="config-box">
                <p class="text"><strong>T2I Training Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Dataset:</strong> BLIP-3o<d-cite key="blip3o"></d-cite> (~28M samples)</li>
                    <li><strong>Resolution:</strong> 256√ó256, 512√ó512</li>
                    <li><strong>Batch size:</strong> 1024 (256√ó256), 448 (512√ó512)</li>
                    <li><strong>Learning rate:</strong> Constant 1√ó10<sup>-4</sup></li>
                    <li><strong>Optimizer:</strong> AdamW<d-cite key="adamw"></d-cite></li>
                    <!-- <li><strong>Training steps:</strong> 100K</li> -->
                    <li><strong>EMA:</strong> Decay 0.9999, per-step update</li>
                </ul>
            </div>

            <p class="text">
                We next evaluate the performance of end-to-end tuned VAEs on various text-to-image generation benchmarks.
            </p>

            <!-- <div class="config-box">
                <p class="text"><strong>End-to-End VAE Tuning Configuration:</strong></p>
                <ul class="text">
                    <li><strong>VAE Models:</strong> FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite></li>
                    <li><strong>Dataset:</strong> ImageNet-256<d-cite key="imagenet"></d-cite></li>
                    <li><strong>Training epochs:</strong> 80 epochs</li>
                    <li><strong>Learning rate:</strong> 2√ó10<sup>-5</sup></li>
                </ul>
            </div> -->

            <!-- <p class="text">
                These end-to-end tuned VAEs are then used in all subsequent text-to-image generation experiments, where they are compared against their original counterparts.
            </p> -->
            
        </div>

        <!-- <hr> -->

        <!-- T2I Setup Section -->
        <!-- <div class="method-block">
            <h1 class="text">T2I Setup</h1>

            <p class="text">
                For our diffusion backbone, we follow the setup in Fuse-DiT<d-cite key="fuse-dit"></d-cite> and adopt a variant of the DiT-3B<d-cite key="dit"></d-cite> architecture with a self-attention-based text conditioning mechanism.
                We use Gemma-2B<d-cite key="gemma"></d-cite> to encode text prompts into contextual embeddings.
                Specifically, text representations are projected to produce key and value states, which are concatenated with the keys and values derived from image latent features before the attention operation.
                The query states come solely from the image features, allowing the model to attend to both visual and textual information simultaneously within a unified self-attention layer.
            </p>

            <p class="text">
                For all generation evaluations and qualitative samples, we use 25 sampling steps with a guidance scale of 6.5.
            </p>
        </div> -->
        
        <hr>

        <!-- Quantitative Results Section -->
        <div id='quantitative' class="motivation-block">
            <h1 class="text">Quantitative Results: T2I Performance</h1>

            <p class="text">
                We evaluate End-to-End Tuned VAEs across multiple benchmarks and training scenarios, demonstrating consistent improvements over baseline VAEs.
                End-to-end tuned VAEs show faster convergence and better final performance across all metrics.
            </p>

            <h3 class="text">End-to-End VAEs Leads to Accelerated T2I Training</h3>

            <!-- <div class="config-box">
                <p class="text"><strong>Training Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Dataset:</strong> JourneyDB<d-cite key="journeydb"></d-cite> (~4M samples)</li>
                    <li><strong>Resolution:</strong> 256√ó256</li>
                    <li><strong>Batch size:</strong> 1024</li>
                    <li><strong>Learning rate:</strong> Constant 1√ó10<sup>-4</sup></li>
                    <li><strong>Optimizer:</strong> AdamW<d-cite key="adamw"></d-cite></li>
                    <li><strong>Training steps:</strong> 100K</li>
                    <li><strong>EMA:</strong> Decay 0.9999, per-step update</li>
                </ul>
            </div> -->

            <div class="sidenote-container">
                <p class="text">
                    We compare training with original VAEs (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>) against their end-to-end tuned counterparts across multiple evaluation benchmarks.
                    End-to-End Tuned VAEs consistently achieve better performance across all metrics, with improvements particularly pronounced on vision-centric benchmarks like MJHQ-30K<d-cite key="mjhq30k"></d-cite> and GenEval<d-cite key="geneval"></d-cite>.
                </p>
                <aside class="sidenote">MJHQ-30K focuses on visual quality while GenEval measures compositional understanding.</aside>
            </div>

            <d-figure id="fig-convergence-100k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-100k-combined.png" alt="Training convergence at 100K steps">
                    <figcaption>
                        <strong>Training convergence at 100K steps.</strong>
                        Comparison of baseline VAEs vs End-to-End Tuned VAEs across three VAE families (FLUX-VAE, SD3.5-VAE, Qwen-Image-VAE) and five benchmarks: COCO30k FID<d-cite key="coco30k"></d-cite>, DPG-Bench<d-cite key="dpgbench"></d-cite>, GenAI-Bench<d-cite key="genaibench"></d-cite>, GenEval<d-cite key="geneval"></d-cite>, and MJHQ30k<d-cite key="mjhq30k"></d-cite>.
                        End-to-end tuned VAEs show consistent improvements across all metrics and VAE architectures.
                    </figcaption>
                </figure>
            </d-figure>

            <!-- <h3 class="text">Extended Training with Full Data (500K Steps)</h3> -->

            <!-- <div class="config-box">
                <p class="text"><strong>Changes from 100K Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Dataset:</strong> BLIP-3o<d-cite key="blip3o"></d-cite> pretraining dataset (~28M samples)</li>
                    <li><strong>Training steps:</strong> 500K</li>
                </ul>
            </div> -->

            <!-- <p class="text">
                With extended training on the full BLIP-3o<d-cite key="blip3o"></d-cite> pretraining dataset, end-to-end tuned VAEs maintain their performance advantages over the original VAEs (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>).
                The consistent improvements across all benchmarks demonstrate that the benefits of end-to-end tuning persist at scale with longer training.
            </p> -->

            <d-figure id="fig-convergence-500k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-fulldata-500k-combined.png" alt="Training convergence at 500K steps with full data">
                    <figcaption>
                        <strong>Training convergence (500K steps).</strong>
                        Extended training confirms sustained improvements across all benchmarks with End-to-End Tuned VAEs.
                    </figcaption>
                </figure>
            </d-figure>
            
            <div class="finding-box">
                <ul>
                    <li><strong>Finding 1:</strong> End-to-End VAEs tuned on just "ImageNet 256√ó256" generalize to T2I; leading to better training performance across various text-to-image generation benchmarks.</li>
                </ul>
            </div>


            <h3 class="text">Comparison with REPA Representation Alignment</h3>

            <p class="text">
                To understand the effectiveness of end-to-end tuning, we compare three approaches at 100K training steps:
                (1) FLUX-VAE baseline without modifications,
                (2) FLUX-VAE with REPA<d-cite key="leng2025repa"></d-cite> representation alignment losses added during T2I training, and
                (3) E2E-FLUX-VAE (ours) with end-to-end tuning but without additional alignment losses.
                The results demonstrate that end-to-end tuning outperforms both the baseline and REPA-enhanced approaches across all benchmarks, achieving superior performance without requiring auxiliary alignment objectives.
            </p>

            <d-figure id="fig-repa-comparison-100k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-100k-combined-bar.png" alt="Comparison of FLUX-VAE, FLUX-VAE+REPA, and E2E-FLUX-VAE at 100K steps">
                    <figcaption>
                        <strong>Performance comparison at 100K steps: E2E-FLUX-VAE vs REPA alignment.</strong>
                        Bar chart comparing three approaches across five benchmarks.
                        E2E-FLUX-VAE (red) outperforms both FLUX-VAE baseline (blue) and FLUX-VAE+REPA with representation alignment (orange), demonstrating that end-to-end tuned VAEs lead to better T2I generation without need for any additional representation alignment losses.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="finding-box">
                <ul>
                    <li><strong>Finding 2:</strong> End-to-End VAEs lead to faster T2I training over baseline REPA without need for any additional representation alignment losses.</li>
                </ul>
            </div>

            <h3 class="text">End-to-End VAEs Generalize to Higher Resolutions</h3>

            <!-- <div class="config-box">
                <p class="text"><strong>T2I Training Config. at 512√ó512 resolution:</strong></p>
                <ul class="text">
                    <li><strong>Initialization:</strong> Resumed from 500K checkpoint trained at 256√ó256 resolution</li>
                    <li><strong>Resolution:</strong> 512√ó512</li>
                    <li><strong>Batch size:</strong> 448</li>
                    <li><strong>Training steps:</strong> 200K additional steps</li>
                </ul>
            </div> -->

            <p class="text">
                We also analyze the generalization of end-to-end tuned VAEs to higher resolutions. We resume training from the 500K checkpoint trained at 256√ó256 resolution and train for an additional 200K steps at 512√ó512 resolution with batch size 448.
                We observe that despite being trained at 256√ó256 resolution on ImageNet<d-cite key="imagenet"></d-cite>, 
                end-to-end tuned VAEs continue to outperform the original VAEs across all benchmarks even when trained at 512√ó512 resolution for T2I generation.
                <!-- After resuming training at higher resolution (512√ó512), end-to-end tuned VAEs continue to outperform the original VAEs across all benchmarks. -->
                <!-- These results confirm that the benefits of end-to-end tuning transfer effectively to higher-resolution generation. -->
                <!-- Notably, even though the end-to-end VAE tuning was performed at 256√ó256 resolution on ImageNet<d-cite key="imagenet"></d-cite>, the performance improvements hold at higher resolutions. -->
            </p>

            <d-figure id="fig-convergence-res512" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-fulldata-resume-res512-200k-combined-bar.png" alt="Performance comparison at 512px resolution">
                    <figcaption>
                        <strong>High-resolution training (512px, 200K steps) - Final performance.</strong>
                        Bar chart showing final performance comparison between FLUX-VAE and E2E-FLUX-VAE after 200K additional steps at 512√ó512 resolution.
                        Performance improvements persist when resuming training at higher resolution, demonstrating that E2E-tuned VAEs generalize effectively across resolutions.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="finding-box">
                <ul>
                    <li><strong>Finding 3:</strong> End-to-End VAEs tuned on ImageNet 256√ó256 generalize for better T2I generation across different resolutions (256√ó256, 512√ó512).</li>
                </ul>
            </div>

        </div>

        <!-- <hr> -->

        <!-- Qualitative Comparison Section -->
        <div id='qualitative' class="spatial-structure-block">
            <!-- <h1 class="text">Qualitative Comparison</h1> -->
            <h3 class="text">Qualitative Comparisons</h3>

            <p class="text">
                For qualitative visualization, we use the 200K-step checkpoint trained at 512√ó512 resolution.
                All images are generated with 25 sampling steps and a guidance scale of 6.5.
                Beyond quantitative metrics, End-to-End Tuned VAEs produce visually superior results compared to baseline FLUX-VAE.
                The generated images show improved detail, better prompt adherence, and more coherent compositions.
                Below we show comparisons for T2I generations using models trained with FLUX-VAE and E2E-Tuned FLUX-VAE (Ours).
            </p>

            <!-- <h3 class="text">Sample Generations</h3>
            <p class="text">
                Below are selected examples from REPA-E-T2I generation.
                The carousel automatically cycles through all 30 samples. Use the controls to navigate or pause.
            </p> -->

            <d-figure id="fig-qualitative-carousel" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <!-- Carousel Container -->
                    <div class="carousel-container" style="position: relative; max-width: 100%; margin: 0 auto;">
                        <!-- Main Image Display -->
                        <div class="carousel-slides" style="position: relative; width: 100%; overflow: hidden;">
                            <img id="carousel-image" data-zoomable="" draggable="false"
                                 src="static/img/qualitative/00.png"
                                 alt="Sample 1"
                                 style="width: 100%; display: block; margin: 0 auto;">
                        </div>

                        <!-- Navigation Controls -->
                        <button class="carousel-btn carousel-prev" onclick="changeSlide(-1)"
                                style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%);
                                       background: rgba(0,0,0,0.5); color: white; border: none;
                                       padding: 15px 20px; cursor: pointer; font-size: 20px; border-radius: 5px; z-index: 10;">
                            &#10094;
                        </button>
                        <button class="carousel-btn carousel-next" onclick="changeSlide(1)"
                                style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%);
                                       background: rgba(0,0,0,0.5); color: white; border: none;
                                       padding: 15px 20px; cursor: pointer; font-size: 20px; border-radius: 5px; z-index: 10;">
                            &#10095;
                        </button>

                        <!-- Counter and Controls -->
                        <div class="carousel-info" style="text-align: center; margin-top: 15px; display: flex; justify-content: center; align-items: center; gap: 20px;">
                            <span id="carousel-counter" style="font-size: 1em; color: #333;">Image 1 / 8</span>
                            <button id="pause-btn" onclick="toggleAutoPlay()"
                                    style="background: #508af6; color: white; border: none;
                                           padding: 8px 16px; cursor: pointer; font-size: 0.9em; border-radius: 5px;">
                                ‚è∏ Pause
                            </button>
                        </div>
                    </div>

                    <figcaption style="margin-top: 20px;">
                        <strong>Qualitative comparison samples (200K steps, 512px resolution).</strong>
                        Models trained with End-to-End Tuned VAEs show improved quality and prompt adherence across diverse prompts compared to FLUX-VAE.
                        Click images to zoom for detail inspection. 
                        <!-- Automatically cycles every 4 seconds. -->
                    </figcaption>
                </figure>
            </d-figure>

            <!-- Carousel JavaScript -->
            <script>
                let currentSlide = 0;
                const totalSlides = 8;
                let autoPlayInterval;
                let isPlaying = true;

                // Generate array of image paths
                const imagePaths = Array.from({length: totalSlides}, (_, i) => {
                    const num = String(i).padStart(2, '0');
                    return `static/img/qualitative/${num}.png`;
                });

                function showSlide(n) {
                    // Wrap around
                    if (n >= totalSlides) {
                        currentSlide = 0;
                    } else if (n < 0) {
                        currentSlide = totalSlides - 1;
                    } else {
                        currentSlide = n;
                    }

                    // Update image
                    const imgElement = document.getElementById('carousel-image');
                    imgElement.src = imagePaths[currentSlide];
                    imgElement.alt = `Sample ${currentSlide + 1}`;

                    // Update counter
                    document.getElementById('carousel-counter').textContent =
                        `Image ${currentSlide + 1} / ${totalSlides}`;
                }

                function changeSlide(direction) {
                    showSlide(currentSlide + direction);
                }

                function startAutoPlay() {
                    autoPlayInterval = setInterval(() => {
                        changeSlide(1);
                    }, 4000); // Change image every 4 seconds
                }

                function stopAutoPlay() {
                    clearInterval(autoPlayInterval);
                }

                function toggleAutoPlay() {
                    const btn = document.getElementById('pause-btn');
                    if (isPlaying) {
                        stopAutoPlay();
                        btn.innerHTML = '‚ñ∂ Play';
                        isPlaying = false;
                    } else {
                        startAutoPlay();
                        btn.innerHTML = '‚è∏ Pause';
                        isPlaying = true;
                    }
                }

                // Pause on hover
                const carouselContainer = document.querySelector('.carousel-container');
                carouselContainer.addEventListener('mouseenter', () => {
                    if (isPlaying) {
                        stopAutoPlay();
                    }
                });
                carouselContainer.addEventListener('mouseleave', () => {
                    if (isPlaying) {
                        startAutoPlay();
                    }
                });

                // Keyboard navigation
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowLeft') {
                        changeSlide(-1);
                    } else if (e.key === 'ArrowRight') {
                        changeSlide(1);
                    } else if (e.key === ' ') {
                        e.preventDefault();
                        toggleAutoPlay();
                    }
                });

                // Start autoplay on page load
                window.addEventListener('load', () => {
                    startAutoPlay();
                });
            </script>

            <style>
                .carousel-btn:hover {
                    background: rgba(0,0,0,0.8) !important;
                }

                #pause-btn:hover {
                    background: #3a6ed8 !important;
                }

                @media (max-width: 768px) {
                    .carousel-btn {
                        padding: 10px 15px !important;
                        font-size: 16px !important;
                    }
                }
            </style>
        </div>

        <!-- <hr> -->

        <!-- VAE Reconstruction Quality Section -->
        <div id='reconstruction' class="spatial-structure-block">
            <h3 class="text">Impact of End-to-End Tuning on VAE Reconstruction Quality</h3>

            <p class="text">
                We also analyze the impact of end-to-end tuning on VAE reconstruction quality.
                Notably, despite being only tuned on ImageNet 256√ó256, end-to-end tuned VAEs show improved generation quality while maintaining reconstruction fidelity across challenging scenes with multiple faces, subjects and text.
            </p>
            <d-figure style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="margin-bottom: 15px;">
                        <img data-zoomable="" draggable="false" src="static/img/reconstruction/cat_repa_e.png" alt="VAE reconstruction comparison - cat with sign">
                    </div>
                    <div style="margin-bottom: 15px;">
                        <img data-zoomable="" draggable="false" src="static/img/reconstruction/children_birthday.png" alt="VAE reconstruction comparison - children with birthday sign">
                    </div>
                    <!-- <div style="margin-bottom: 15px;">
                        <img data-zoomable="" draggable="false" src="static/img/reconstruction/friends_photo.png" alt="VAE reconstruction comparison - friends posing for photo">
                    </div> -->
                    <div style="margin-bottom: 15px;">
                        <img data-zoomable="" draggable="false" src="static/img/reconstruction/text_v1.png" alt="VAE reconstruction comparison - textbook study scene">
                    </div>
                    <div style="margin-bottom: 15px;">
                        <img data-zoomable="" draggable="false" src="static/img/reconstruction/children_8_photo.png" alt="VAE reconstruction comparison - 8 children posing">
                    </div>
                    <figcaption>
                        <strong>Reconstruction results across all E2E-tuned VAEs.</strong>
                        End-to-End Tuned VAEs show improved generation quality while maintaining reconstruction fidelity across challenging scenes with multiple faces, subjects and text.
                        Click images to zoom for detail inspection.
                    </figcaption>
                </figure>
            </d-figure>
            <div class="finding-box">
                <ul>
                    <li><strong>Finding 4:</strong> End-to-End VAEs despite being only tuned on ImageNet 256√ó256 improve generation performance while maintaining reconstruction fidelity across challenging scenes with multiple faces, subjects and text.</li>
                </ul>
            </div>
        </div>

        <hr>

        <!-- Latent Space Analysis Section -->
        <div id='latent-analysis' class="method-block">
            <h1 class="text">End-to-End Tuned VAEs Lead to Better Latent Space Structure</h1>

            <p class="text">
                To understand what makes End-to-End Tuned VAEs effective, we analyze the learned latent representations through PCA projections and spatial similarity analysis.
                These visualizations reveal how end-to-end tuning shapes the VAE's latent space to better support high-quality generation.
                Notably, end-to-end tuning enriches the VAE latent space by incorporating more structural and semantic information over traditional VAEs (Flux-VAE, SDVAE) trained for reconstruction alone. Please also refer the original REPA-E<d-cite key="leng2025repa"></d-cite> paper for more details.
            </p>

            <h3 class="text">PCA Projections of VAE Latents</h3>
            <p class="text">
                We project VAE latent representations to 2D using PCA and visualize them as RGB images (first 3 principal components).
                This reveals the spatial structure and semantic organization learned by different VAE architectures.
                As illustrated in the PCA visualizations, end-to-end training injects additional structural and semantic information into the latent representations.
            </p>

            <d-figure id="fig-pca-viz" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 10px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_0.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_3.png">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_6.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_7.png">
                        </div>
                    </div>
                    <figcaption>
                        <strong>PCA projection visualization.</strong>
                        Comparison of latent space structure between baseline FLUX-VAE and E2E-Tuned Flux-VAE.
                        Colors represent the first 3 principal components depicted as RGB for visualization. We observe that E2E tuned Flux-VAE shows more semantic and structural information in the latent space compared to baseline FLUX-VAE.
                    </figcaption>
                </figure>
            </d-figure>

            <h3 class="text">Spatial Self-Similarity Analysis</h3>
            <p class="text">
                We also analyze the cosine similarity between patch tokens in the latent space to measure spatial structure.
                End-to-End Tuned VAEs show more coherent spatial patterns, indicating better capture of local and global image structure.
                As shown in the similarity maps, end-to-end tuning embeds more meaningful structural and semantic relationships between patches, making the latent space more informative for the diffusion model to generate high-quality images.
            </p>

            <d-figure id="fig-similarity-viz" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 15px; margin-bottom: 20px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_0.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_2.png">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 15px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_4.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_6.png">
                        </div>
                    </div>
                    <figcaption>
                        <strong>Spatial self-similarity heatmaps.</strong>
                        Cosine similarity between latent patch tokens reveals spatial structure.
                        E2E-Tuned FLUX-VAE shows more coherent patterns indicating better structural representation.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="finding-box">
                <ul>
                    <li><strong>Finding 5:</strong> End-to-End tuned VAEs show improved semantic spatial structure and details over FLUX-VAE, as evidenced by PCA projections and spatial self-similarity analysis.</li>
                </ul>
            </div>
        </div>

        <hr>
        
        <!-- ImageNet Generalization Section -->
        <div id='imagenet' class="results-block">
            <h1 class="text">ImageNet Generalization</h1>

            <p class="text">
                Finally, we show that the end-to-end tuned VAEs can also be used for traditional image generation benchmarks like ImageNet<d-cite key="imagenet"></d-cite> 256x256.
                We observe that the end-to-end tuned VAEs achieve significantly better FID scores (better generation quality) compared to their original counterparts across all VAE families (FLUX<d-cite key="flux"></d-cite>, SD3.5<d-cite key="sd3.5"></d-cite>, Qwen<d-cite key="qwen-image"></d-cite>).
                <!-- A key question is whether VAEs optimized for T2I generation maintain their ability to encode natural images.
                We evaluate our REPA-E-T2I VAEs on ImageNet reconstruction tasks, measuring FID scores to assess encoding/decoding quality. -->
            </p>

            <d-figure id="fig-imagenet-convergence" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/e2e_convergence_fid.png" alt="ImageNet FID convergence" style="width: 100%;">
                    <figcaption>
                        <strong>ImageNet Results.</strong>
                        Comparison of Standard VAE vs E2E-tuned VAE across FLUX, Qwen, and SD3.5 architectures.
                        End-to-end tuned VAEs achieve significantly better gFID scores.
                    </figcaption>
                </figure>
            </d-figure>

            <!-- <div class="finding-box">
                <ul>
                    <li><strong>Finding 5:</strong> End-to-End VAEs improve generation performance while maintaining reconstruction fidelity across all VAE families (FLUX, SD3.5, Qwen-Image).</li>
                </ul>
            </div> -->

            <!-- <p class="text">
                <strong>Key findings:</strong>
                <ul class="text">
                    <li>E2E-tuned VAEs achieve <strong>substantially lower ImageNet FID scores</strong> across all architectures (roughly 45-50% improvement)</li>
                    <li>The improvements are consistent across different VAE architectures (FLUX, Qwen-Image, SD3.5)</li>
                    <li>End-to-end training enhances both T2I generation <em>and</em> natural image reconstruction capabilities</li>
                </ul>
            </p>

            <p class="text">
                This demonstrates that REPA-E-T2I VAEs serve as general-purpose image encoders, not just specialized T2I components.
                The joint optimization with diffusion transformers leads to representations that excel at both tasks.
            </p> -->
        </div>

        <!-- ImageNet Generalization Section
        <div id='imagenet' class="results-block">
            <h1 class="text">ImageNet Generalization</h1>

            <p class="text">
                A key question is whether VAEs optimized for T2I generation maintain their ability to encode natural images.
                We evaluate our REPA-E-T2I VAEs on ImageNet reconstruction tasks, measuring FID scores to assess encoding/decoding quality.
            </p>

            <d-figure id="fig-imagenet-convergence" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/e2e_convergence_fid.png" alt="ImageNet FID convergence" style="width: 100%;">
                    <figcaption>
                        <strong>ImageNet reconstruction quality (FID scores).</strong>
                        Comparison of Standard VAE vs E2E-tuned VAE across FLUX, Qwen, and SD3.5 architectures.
                        End-to-end tuned VAEs achieve significantly lower FID scores (6.1 vs 11.9 for FLUX, 5.2 vs 9.2 for Qwen, 5.2 vs 9.5 for SD3.5),
                        demonstrating improved reconstruction quality on ImageNet despite being optimized for T2I generation.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Key findings:</strong>
                <ul class="text">
                    <li>E2E-tuned VAEs achieve <strong>substantially lower ImageNet FID scores</strong> across all architectures (roughly 45-50% improvement)</li>
                    <li>The improvements are consistent across different VAE architectures (FLUX, Qwen-Image, SD3.5)</li>
                    <li>End-to-end training enhances both T2I generation <em>and</em> natural image reconstruction capabilities</li>
                </ul>
            </p>

            <p class="text">
                This demonstrates that REPA-E-T2I VAEs serve as general-purpose image encoders, not just specialized T2I components.
                The joint optimization with diffusion transformers leads to representations that excel at both tasks.
            </p>
        </div> -->

        <hr>

        <!-- Conclusion -->
        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Limitations and Future Work</h2>
            <p class="text">
                In this work, we show how end-to-end tuned VAEs lead to better T2I training over more standard VAEs like Flux-VAE and SDVAE which primarily trained for reconstruction alone.
                This primarily happens because end-to-end tuned VAEs lead to better semantic representations while maintaining strong reconstruction fidelity.
                <br><br>
                In future, we are actively studying the impact of end-to-end tuned VAEs for other downstream tasks which are also reliant on both the semantic and reconstrcution ability of the VAE latents. Examples include Image-to-Image Translation, Image-editing, Image inpainitng etc.
            </p>
        </div>

        <!-- Conclusion -->
        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                End-to-end VAEs show superior performance over their original counterparts across all benchmarks (COCO30k<d-cite key="coco30k"></d-cite>, DPG-Bench<d-cite key="dpgbench"></d-cite>, GenAI-Bench<d-cite key="genaibench"></d-cite>, GenEval<d-cite key="geneval"></d-cite>, MJHQ30k<d-cite key="mjhq30k"></d-cite>) without need for any additional representation alignment losses.
                <br><br>
                We hope REPA-E for T2I will inspire further research into end-to-end training strategies for generative models and the co-design of
                VAE/RAE architectures with diffusion transformers.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>Team</h3>
            <p style="font-size: 0.9em;">
                This work is a joint collaboration between the REPA-E team and Canva.
            </p>

            <p style="font-size: 0.9em; margin-top: 10px;">
                <strong>REPA-E Team:</strong>
                <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian Leng<sup>*</sup></a>,
                <a href="https://1jsingh.github.io/" target="_blank">Jaskirat Singh<sup>*</sup></a>,
                <a href="https://hou-yz.github.io/" target="_blank">Yunzhong Hou</a>,
                <a href="https://people.csiro.au/X/Z/Zhenchang-Xing/" target="_blank">Zhenchang Xing</a>,
                <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,
                <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang Zheng</a>
            </p>

            <p style="font-size: 0.9em; margin-top: 10px;">
                <strong>Canva Team:</strong>
                <!-- <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian Leng</a>, -->
                <a href="https://rynmurdock.github.io/" target="_blank">Ryan Murdock</a>,
                <a href="https://www.ethansmith2000.com/" target="_blank">Ethan Smith</a>,
                <a href="https://xiaoyang-rebecca.github.io/cv/" target="_blank">Rebecca Li</a>
                <!-- <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang Zheng</a> -->
            </p>

            <h3>BibTeX</h3>
            <div class="bibtex-container" style="position: relative;">
                <button class="copy-btn" id="copy-bibtex-btn" onclick="copyBibtex()" style="
                    position: absolute;
                    top: 10px;
                    right: 10px;
                    padding: 5px 10px;
                    background: #508af6;
                    color: white;
                    border: none;
                    border-radius: 4px;
                    cursor: pointer;
                    font-size: 0.85em;
                    opacity: 0;
                    transition: opacity 0.3s ease;
                    z-index: 10;
                ">üìã Copy</button>
                <p class="bibtex" id="bibtex-content">
                    @article{leng2025repae,<br>
                    &nbsp;&nbsp;title={{REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers}},<br>
                    &nbsp;&nbsp;author={Leng, Xingjian and Singh, Jaskirat and Hou, Yunzhong and Xing, Zhenchang and Xie, Saining and Zheng, Liang},<br>
                    &nbsp;&nbsp;journal={arXiv preprint arXiv:2504.10483},<br>
                    &nbsp;&nbsp;year={2025}<br>
                    }
                </p>
            </div>

            <!-- <h3>References</h3> -->
            <!-- Add BibTeX entries to references.bib file in the same directory -->
            <!-- Use <d-cite key="citation-key"></d-cite> in the text to cite papers -->
            <!-- Citations will appear as numbered references [1], [2], etc. -->
            <d-bibliography src="references.bib"></d-bibliography>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        <script src="./static/js/nav-bar.js"></script>

        <!-- Copy BibTeX functionality -->
        <style>
            .bibtex-container:hover .copy-btn {
                opacity: 1 !important;
            }

            .copy-btn:hover {
                background: #3a6ed8 !important;
                transform: translateY(-1px);
                box-shadow: 0 2px 4px rgba(0,0,0,0.2);
            }

            .copy-btn.copied {
                background: #4caf50 !important;
            }
        </style>

        <script>
            function copyBibtex() {
                const bibtexElement = document.getElementById('bibtex-content');
                const bibtexText = bibtexElement.innerText || bibtexElement.textContent;

                if (navigator.clipboard && navigator.clipboard.writeText) {
                    navigator.clipboard.writeText(bibtexText).then(() => {
                        showCopySuccess();
                    }).catch(err => {
                        copyWithFallback(bibtexText);
                    });
                } else {
                    copyWithFallback(bibtexText);
                }
            }

            function copyWithFallback(text) {
                const textarea = document.createElement('textarea');
                textarea.value = text;
                textarea.style.position = 'fixed';
                textarea.style.opacity = '0';
                document.body.appendChild(textarea);
                textarea.select();

                try {
                    document.execCommand('copy');
                    showCopySuccess();
                } catch (err) {
                    console.error('Failed to copy:', err);
                }

                document.body.removeChild(textarea);
            }

            function showCopySuccess() {
                const button = document.getElementById('copy-bibtex-btn');
                const originalText = button.innerHTML;

                button.innerHTML = '‚úì Copied!';
                button.classList.add('copied');

                setTimeout(() => {
                    button.innerHTML = originalText;
                    button.classList.remove('copied');
                }, 2000);
            }
        </script>

        <!-- TOC scroll highlighting and visibility -->
        <script>
        document.addEventListener('DOMContentLoaded', function() {
            const tocContainer = document.querySelector('.toc-container');
            const tocLinks = document.querySelectorAll('.toc-container a');
            const sections = ['quickstart', 'training', 'quantitative', 'latent-analysis', 'imagenet', 'conclusion'];
            const headerWrapper = document.querySelector('.header-wrapper');

            function updateTOC() {
                const scrollPos = window.scrollY;
                const headerBottom = headerWrapper ? headerWrapper.offsetTop + headerWrapper.offsetHeight : 400;

                // Show/hide TOC based on scroll position
                if (scrollPos > headerBottom - 100) {
                    tocContainer.classList.add('visible');
                } else {
                    tocContainer.classList.remove('visible');
                }

                // Update active link
                let current = '';
                const scrollPosForActive = scrollPos + 150;

                sections.forEach(id => {
                    const section = document.getElementById(id);
                    if (section && scrollPosForActive >= section.offsetTop) {
                        current = id;
                    }
                });

                tocLinks.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href') === '#' + current) {
                        link.classList.add('active');
                    }
                });
            }

            window.addEventListener('scroll', updateTOC);
            updateTOC();
        });
        </script>
    </body>
</html>
