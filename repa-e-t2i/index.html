<!doctype html>
<html lang="en">
    <head>
        <title>REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers</title>
        <link rel="icon" type="image/svg+xml" href="./static/img/icons/favicon.svg">
        <link rel="alternate icon" href="./static/img/icons/jellyfish.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://end2end-diffusion.github.io/repa-e-t2i" />
        <meta property="og:title" content="REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers" />
        <meta property="og:description" content="End-to-end tunable VAEs for improved text-to-image generation with diffusion transformers. Better performance across metrics with maintained ImageNet generalization." />

        <!-- Twitter -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers" />
        <meta name="twitter:description" content="End-to-end tunable VAEs for improved text-to-image generation with diffusion transformers." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script defer="" src="./static/js/hider.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/custom.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- medium zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">REPA-E <em>for</em> T2I</h1>
                    <h2>Family of end-to-end tuned VAEs for <em>supercharging</em><br>T2I diffusion transformers</h2>

                    <p>
                        We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                        <strong>End-to-end VAEs show superior performance over their original counterparts</strong> across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) without need for any additional representation alignment losses.
                    </p>

                    <!-- Key Points Icon Container -->
                    <div class="icon-container">
                        <div class="icon-item">
                            <img src="./static/img/icons/accuracy.svg" alt="Performance Icon">
                            <div><strong>Better T2I Performance</strong>: Consistent improvements over baseline VAEs across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) with end-to-end tuned VAEs.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/spatial.svg" alt="Quality Icon">
                            <div><strong>Family of End-to-End Tuned VAEs</strong>: We release a family of end-to-end tuned VAEs across different families (FLUX-VAE, SD3.5-VAE, Qwen-Image-VAE) for supercharging text-to-image generation training.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/irepa.svg" alt="Generalization Icon">
                            <div><strong>Better Latent Space Structure</strong>: End-to-end tuned VAEs show better latent space structure compared to their original counterparts which are mostly optimized for reconstruction.</div>
                        </div>
                    </div>

                    <div class="button-container">
                        <a href="https://arxiv.org/abs/2504.10483" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            REPA-E Paper
                        </a>
                        <a href="https://github.com/End2End-Diffusion/REPA-E" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="https://huggingface.co/REPA-E" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Models</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <!-- Placeholder for hero image -->
                    <img draggable="false" src="e2e-v2.png" alt="REPA-E-T2I Sample" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <!-- <div class="byline">
            <div class="byline-container">
                <p align="center">
                    <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian&nbsp;Leng</a><sup>1,2*</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://1jsingh.github.io/" target="_blank">Jaskirat&nbsp;Singh</a><sup>1</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://rynmurdock.github.io/" target="_blank">Ryan&nbsp;Murdock</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://www.ethansmith2000.com/" target="_blank">Ethan&nbsp;Smith</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://xiaoyang-rebecca.github.io/cv/" target="_blank">Rebecca&nbsp;Li</a><sup>2</sup> &ensp; <b>&middot;</b> &ensp;
                    <a href="https://www.sainingxie.com/" target="_blank">Saining&nbsp;Xie</a><sup>3</sup>&ensp; <b>&middot;</b> &ensp;
                    <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang&nbsp;Zheng</a><sup>1</sup>&ensp;
                </p>
                <p align="center">
                    <sup>1</sup> Australian National University &emsp; <sup>2</sup>Canva &emsp; <sup>3</sup>New York University &emsp; <br>
                    <sub><sup>*</sup>Done during internship at Canva &emsp;</sub>
                </p>
            </div>
        </div> -->

        <!-- Collaboration Heading -->
        <div style="text-align: center; margin: 40px 0 40px 0;">
            <!-- Logo and collaboration line (largest) -->
            <div style="display: flex;
                        align-items: center;
                        justify-content: center;
                        gap: 20px;
                        margin-bottom: 15px;">
                <img src="./static/img/logos/repa-e-logo.png" alt="REPA-E"
                     style="height: 60px;">
                <h2 style="color: #6c757d;
                           font-weight: 300;
                           margin: 0;
                           font-size: 1.8em;
                           letter-spacing: 1px;
                           font-family: 'Georgia', 'Times New Roman', serif;">
                    <a href="https://github.com/End2End-Diffusion/REPA-E" target="_blank" style="color: #6c757d; text-decoration: none;">REPA-E</a> <span style="filter: grayscale(100%);">ü§ù</span> <a href="https://x.com/canva" target="_blank" style="color: #6c757d; text-decoration: none;">Canva</a>
                </h2>
                <img src="./static/img/logos/canva-logo.png" alt="Canva"
                     style="height: 60px;">
            </div>

            <!-- "presents" line (smaller) -->
            <p style="color: #6c757d;
                      font-weight: 300;
                      margin: 10px 0;
                      font-size: 1.1em;
                      letter-spacing: 0.5px;
                      font-family: 'Georgia', 'Times New Roman', serif;">
                presents
            </p>

            <!-- Subtitle line (medium) -->
            <p style="color: #6c757d;
                      font-weight: 400;
                      margin: 10px 0 0 0;
                      font-size: 1.2em;
                      letter-spacing: 0.3px;
                      line-height: 1.4;
                      white-space: nowrap;
                      font-family: 'Georgia', 'Times New Roman', serif;">
                Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers
            </p>
        </div>

        <hr style="width: 60%; margin: 20px auto; border: none; border-top: 1px solid #d0d0d0;">

        <p class="text">
            We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
            End-to-end VAEs show superior performance across all benchmarks (COCO30k<d-cite key="coco30k"></d-cite>, DPG-Bench<d-cite key="dpgbench"></d-cite>, GenAI-Bench<d-cite key="genaibench"></d-cite>, GenEval<d-cite key="geneval"></d-cite>, MJHQ30k<d-cite key="mjhq30k"></d-cite>) without need for any additional representation alignment losses!
            <!-- Beyond T2I improvements, we show that end-to-end tuning enhances latent space structure and maintains strong ImageNet<d-cite key="imagenet"></d-cite> generalization. -->
        </p>

        <p class="text">
            <!-- Through comprehensive evaluation across multiple scales, datasets, and resolutions, we reveal four key findings: -->
            <ol class="text">
                <li><strong><a href="#quantitative">&sect;Better T2I Performance</a></strong>: End-to-End tuned VAEs lead to better T2I generation without need for any additional representation alignment losses!</li>
                <li><strong><a href="#quantitative">&sect;End-to-End Training on ImageNet Generalizes to T2I</a></strong>: End-to-End VAEs tuned on just "ImageNet 256√ó256" <d-cite key="imagenet"></d-cite> generalize for better "T2I generation" across different resolutions (256√ó256, 512√ó512).</li>
                <li><strong><a href="#latent-analysis">&sect;Improved Latent Space Structure</a></strong>: End-to-End tuned VAEs show improved semantic spatial structure and details over traditionally used VAEs like FLUX-VAE, SD3.5-VAE, Qwen-Image-VAE etc.</li>
                <li><strong><a href="#imagenet">&sect;SOTA performance on Imagenet</a></strong>: End-to-End tuned VAEs achieve new state-of-the-art performance on ImageNet 256√ó256 <d-cite key="imagenet"></d-cite> achieving gFID 1.12 with classifer-free guidance.</li>
            </ol>
        </p>

        <!-- Jump to Sections -->
        <div class="icon-row">
            <a href="#quantitative" class="icon-link">
                <img src="./static/img/icons/accuracy-blue.svg" alt="Quantitative" class="icon">
                Quantitative<br>Results (T2I)
            </a>
            <a href="#qualitative" class="icon-link">
                <img src="./static/img/icons/spatial-blue.svg" alt="Qualitative" class="icon">
                Qualitative<br>Comparison
            </a>
            <a href="#latent-analysis" class="icon-link">
                <img src="./static/img/icons/irepa-blue.svg" alt="Latent Analysis" class="icon">
                Latent Space<br>Analysis
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>

        <hr>

        <!-- End-to-End VAE Training Section -->
        <div class="method-block">
            <h1 class="text">End-to-End VAE Training Recipe and T2I Setup</h1>

            <p class="text">
                We perform end-to-end tuning on ImageNet 256√ó256.  <d-cite key="imagenet"></d-cite> to obtained end-to-end tuned encoders for popular VAE families like FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>. 
                We then compare the performance of obtained end-to-end tuned VAEs with the standard VAEs (e.g, Flux-VAE) for text-to-image (T2I) generation tasks.
                <!-- Before moving on to text-to-image generation tasks, we first perform end-to-end tuning to VAEs.
                We use the official REPA-E<d-cite key="leng2025repa"></d-cite> implementation with the same hyperparameter as the original paper, except for the learning rate.
                Since we are fine-tuning already high-quality VAEs (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>), we opt for a smaller learning rate at 2√ó10<sup>-5</sup>. -->
            </p>
            
            <p class="text">
                <strong>End-to-End VAE Tuning on ImageNet 256√ó256</strong> We follow the same training recipe as the original REPA-E<d-cite key="leng2025repa"></d-cite> paper, for end-to-end tuning on ImageNet 256√ó256.
                We use a small learning rate of 2√ó10<sup>-5</sup>, AdamW optimizer and 80 epochs for end-to-end tuning across all VAE families (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>). We next refer the corresponding finetuned VAEs as E2E-FLUX-VAE<d-cite key="e2e-flux"></d-cite>, E2E-SD-3.5-VAE<d-cite key="e2e-sd3.5"></d-cite>, E2E-Qwen-Image-VAE<d-cite key="e2e-qwen-image"></d-cite> respectively.
            </p>
            <div class="config-box">
                <p class="text"><strong>End-to-End VAE Tuning Configuration:</strong></p>
                <ul class="text">
                    <li><strong>VAE Models:</strong> FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite></li>
                    <li><strong>Dataset:</strong> ImageNet-256<d-cite key="imagenet"></d-cite></li>
                    <li><strong>Training epochs:</strong> 80 epochs</li>
                    <li><strong>Learning rate:</strong> 2√ó10<sup>-5</sup></li>
                </ul>
            </div>

            <p class="text">
                <strong>T2I Training Setup.</strong> For our diffusion backbone, we follow the setup in Fuse-DiT<d-cite key="fuse-dit"></d-cite> and adopt a variant of the DiT-3B<d-cite key="dit"></d-cite> architecture with a self-attention-based text conditioning mechanism.
                We use Gemma-2B<d-cite key="gemma"></d-cite> to encode text prompts into contextual embeddings.
                <!-- Specifically, text representations are projected to produce key and value states, which are concatenated with the keys and values derived from image latent features before the attention operation. -->
                <!-- The query states come solely from the image features, allowing the model to attend to both visual and textual information simultaneously within a unified self-attention layer. -->
                For training we use the BLIP-3o<d-cite key="blip3o"></d-cite> pretraining dataset (~28M samples) for T2I training with both original and end-to-end tuned VAEs. We perform experiments at both 256√ó256 and 512√ó512 resolutions. Unless otherwise specified, we use 25 sampling steps with a guidance scale of 6.5 for inference.
            </p>

            <div class="config-box">
                <p class="text"><strong>T2I Training Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Dataset:</strong> BLIP-3o<d-cite key="blip3o"></d-cite> (~28M samples)</li>
                    <li><strong>Resolution:</strong> 256√ó256, 512√ó512</li>
                    <li><strong>Batch size:</strong> 1024 (256√ó256), 448 (512√ó512)</li>
                    <li><strong>Learning rate:</strong> Constant 1√ó10<sup>-4</sup></li>
                    <li><strong>Optimizer:</strong> AdamW<d-cite key="adamw"></d-cite></li>
                    <!-- <li><strong>Training steps:</strong> 100K</li> -->
                    <li><strong>EMA:</strong> Decay 0.9999, per-step update</li>
                </ul>
            </div>

            <!-- <div class="config-box">
                <p class="text"><strong>End-to-End VAE Tuning Configuration:</strong></p>
                <ul class="text">
                    <li><strong>VAE Models:</strong> FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite></li>
                    <li><strong>Dataset:</strong> ImageNet-256<d-cite key="imagenet"></d-cite></li>
                    <li><strong>Training epochs:</strong> 80 epochs</li>
                    <li><strong>Learning rate:</strong> 2√ó10<sup>-5</sup></li>
                </ul>
            </div> -->

            <!-- <p class="text">
                These end-to-end tuned VAEs are then used in all subsequent text-to-image generation experiments, where they are compared against their original counterparts.
            </p> -->
        </div>

        <!-- <hr> -->

        <!-- T2I Setup Section -->
        <!-- <div class="method-block">
            <h1 class="text">T2I Setup</h1>

            <p class="text">
                For our diffusion backbone, we follow the setup in Fuse-DiT<d-cite key="fuse-dit"></d-cite> and adopt a variant of the DiT-3B<d-cite key="dit"></d-cite> architecture with a self-attention-based text conditioning mechanism.
                We use Gemma-2B<d-cite key="gemma"></d-cite> to encode text prompts into contextual embeddings.
                Specifically, text representations are projected to produce key and value states, which are concatenated with the keys and values derived from image latent features before the attention operation.
                The query states come solely from the image features, allowing the model to attend to both visual and textual information simultaneously within a unified self-attention layer.
            </p>

            <p class="text">
                For all generation evaluations and qualitative samples, we use 25 sampling steps with a guidance scale of 6.5.
            </p>
        </div> -->

        <hr>

        <!-- Quantitative Results Section -->
        <div id='quantitative' class="motivation-block">
            <h1 class="text">Quantitative Results: T2I Performance</h1>

            <p class="text">
                We evaluate End-to-End Tuned VAEs across multiple benchmarks and training scenarios, demonstrating consistent improvements over baseline VAEs.
                End-to-end tuned VAEs show faster convergence and better final performance across all metrics.
            </p>

            <h3 class="text">End-to-End VAEs Leads to Accelerated T2I Training</h3>

            <!-- <div class="config-box">
                <p class="text"><strong>Training Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Dataset:</strong> JourneyDB<d-cite key="journeydb"></d-cite> (~4M samples)</li>
                    <li><strong>Resolution:</strong> 256√ó256</li>
                    <li><strong>Batch size:</strong> 1024</li>
                    <li><strong>Learning rate:</strong> Constant 1√ó10<sup>-4</sup></li>
                    <li><strong>Optimizer:</strong> AdamW<d-cite key="adamw"></d-cite></li>
                    <li><strong>Training steps:</strong> 100K</li>
                    <li><strong>EMA:</strong> Decay 0.9999, per-step update</li>
                </ul>
            </div> -->

            <p class="text">
                We compare training with original VAEs (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>) against their end-to-end tuned counterparts across multiple evaluation benchmarks.
                End-to-End Tuned VAEs consistently achieve better performance across all metrics, with improvements particularly pronounced on vision-centric benchmarks like MJHQ-30K<d-cite key="mjhq30k"></d-cite> and GenEval<d-cite key="geneval"></d-cite>.
            </p>

            <d-figure id="fig-convergence-100k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-100k-combined.png" alt="Training convergence at 100K steps">
                    <figcaption>
                        <strong>Training convergence at 100K steps.</strong>
                        Comparison of baseline VAEs vs End-to-End Tuned VAEs across three VAE families (FLUX-VAE, SD3.5-VAE, Qwen-Image-VAE) and five benchmarks: COCO30k FID<d-cite key="coco30k"></d-cite>, DPG-Bench<d-cite key="dpgbench"></d-cite>, GenAI-Bench<d-cite key="genaibench"></d-cite>, GenEval<d-cite key="geneval"></d-cite>, and MJHQ30k<d-cite key="mjhq30k"></d-cite>.
                        End-to-end tuned VAEs show consistent improvements across all metrics and VAE architectures.
                    </figcaption>
                </figure>
            </d-figure>

            <!-- <h3 class="text">Extended Training with Full Data (500K Steps)</h3> -->

            <!-- <div class="config-box">
                <p class="text"><strong>Changes from 100K Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Dataset:</strong> BLIP-3o<d-cite key="blip3o"></d-cite> pretraining dataset (~28M samples)</li>
                    <li><strong>Training steps:</strong> 500K</li>
                </ul>
            </div> -->

            <!-- <p class="text">
                With extended training on the full BLIP-3o<d-cite key="blip3o"></d-cite> pretraining dataset, end-to-end tuned VAEs maintain their performance advantages over the original VAEs (FLUX-VAE<d-cite key="flux"></d-cite>, SD-3.5-VAE<d-cite key="sd3.5"></d-cite>, Qwen-Image-VAE<d-cite key="qwen-image"></d-cite>).
                The consistent improvements across all benchmarks demonstrate that the benefits of end-to-end tuning persist at scale with longer training.
            </p> -->

            <d-figure id="fig-convergence-500k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-fulldata-500k-combined.png" alt="Training convergence at 500K steps with full data">
                    <figcaption>
                        <strong>Training convergence (500K steps).</strong>
                        Extended training confirms sustained improvements across all benchmarks with End-to-End Tuned VAEs.
                    </figcaption>
                </figure>
            </d-figure>
            
            <h3 class="text">Comparison with REPA Representation Alignment</h3>

            <p class="text">
                To understand the effectiveness of end-to-end tuning, we compare three approaches at 100K training steps:
                (1) FLUX-VAE baseline without modifications,
                (2) FLUX-VAE with REPA<d-cite key="leng2025repa"></d-cite> representation alignment losses added during T2I training, and
                (3) E2E-FLUX-VAE (ours) with end-to-end tuning but without additional alignment losses.
                The results demonstrate that end-to-end tuning outperforms both the baseline and REPA-enhanced approaches across all benchmarks, achieving superior performance without requiring auxiliary alignment objectives.
            </p>

            <d-figure id="fig-repa-comparison-100k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-100k-combined-bar.png" alt="Comparison of FLUX-VAE, FLUX-VAE+REPA, and E2E-FLUX-VAE at 100K steps">
                    <figcaption>
                        <strong>Performance comparison at 100K steps: E2E-FLUX-VAE vs REPA alignment.</strong>
                        Bar chart comparing three approaches across five benchmarks.
                        E2E-FLUX-VAE (red) outperforms both FLUX-VAE baseline (blue) and FLUX-VAE+REPA with representation alignment (orange), demonstrating that end-to-end tuned VAEs lead to better T2I generation without need for any additional representation alignment losses.
                    </figcaption>
                </figure>
            </d-figure>

            <h3 class="text">Generalization to Higher Resolutions</h3>

            <div class="config-box">
                <p class="text"><strong>Changes from 500K Configuration:</strong></p>
                <ul class="text">
                    <li><strong>Initialization:</strong> Resumed from 500K checkpoint trained at 256√ó256 resolution</li>
                    <li><strong>Resolution:</strong> 512√ó512</li>
                    <li><strong>Batch size:</strong> 448</li>
                    <li><strong>Training steps:</strong> 200K additional steps</li>
                </ul>
            </div>

            <p class="text">
                After resuming training at higher resolution (512√ó512), end-to-end tuned VAEs continue to outperform the original VAEs across all benchmarks.
                These results confirm that the benefits of end-to-end tuning transfer effectively to higher-resolution generation.
                Notably, even though the end-to-end VAE tuning was performed at 256√ó256 resolution on ImageNet<d-cite key="imagenet"></d-cite>, the performance improvements hold at higher resolutions.
            </p>

            <d-figure id="fig-convergence-res512" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/3b-t2i-convergence-fulldata-resume-res512-200k-combined-bar.png" alt="Performance comparison at 512px resolution">
                    <figcaption>
                        <strong>High-resolution training (512px, 200K steps) - Final performance.</strong>
                        Bar chart showing final performance comparison between FLUX-VAE and E2E-FLUX-VAE after 200K additional steps at 512√ó512 resolution.
                        Performance improvements persist when resuming training at higher resolution, demonstrating that E2E-tuned VAEs generalize effectively across resolutions.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="finding-box">
                <ul>
                    <li><strong>Key Finding:</strong> End-to-End VAEs tuned on ImageNet 256√ó256 generalize for better T2I generation across different resolutions (256√ó256, 512√ó512).</li>
                </ul>
            </div>

        </div>

        <hr>

        <!-- Qualitative Comparison Section -->
        <div id='qualitative' class="spatial-structure-block">
            <h1 class="text">Qualitative Comparison</h1>

            <p class="text">
                For qualitative visualization, we use the 200K-step checkpoint trained at 512√ó512 resolution.
                All images are generated with 25 sampling steps and a guidance scale of 6.5.
                Beyond quantitative metrics, End-to-End Tuned VAEs produce visually superior results compared to baseline FLUX-VAE.
                The generated images show improved detail, better prompt adherence, and more coherent compositions.
                Below we show comparisons for T2I generations using models trained with FLUX-VAE and E2E-Tuned FLUX-VAE (Ours).
            </p>

            <!-- <h3 class="text">Sample Generations</h3>
            <p class="text">
                Below are selected examples from REPA-E-T2I generation.
                The carousel automatically cycles through all 30 samples. Use the controls to navigate or pause.
            </p> -->

            <d-figure id="fig-qualitative-carousel" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <!-- Carousel Container -->
                    <div class="carousel-container" style="position: relative; max-width: 100%; margin: 0 auto;">
                        <!-- Main Image Display -->
                        <div class="carousel-slides" style="position: relative; width: 100%; overflow: hidden;">
                            <img id="carousel-image" data-zoomable="" draggable="false"
                                 src="static/img/qualitative/00.png"
                                 alt="Sample 1"
                                 style="width: 100%; display: block; margin: 0 auto;">
                        </div>

                        <!-- Navigation Controls -->
                        <button class="carousel-btn carousel-prev" onclick="changeSlide(-1)"
                                style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%);
                                       background: rgba(0,0,0,0.5); color: white; border: none;
                                       padding: 15px 20px; cursor: pointer; font-size: 20px; border-radius: 5px; z-index: 10;">
                            &#10094;
                        </button>
                        <button class="carousel-btn carousel-next" onclick="changeSlide(1)"
                                style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%);
                                       background: rgba(0,0,0,0.5); color: white; border: none;
                                       padding: 15px 20px; cursor: pointer; font-size: 20px; border-radius: 5px; z-index: 10;">
                            &#10095;
                        </button>

                        <!-- Counter and Controls -->
                        <div class="carousel-info" style="text-align: center; margin-top: 15px; display: flex; justify-content: center; align-items: center; gap: 20px;">
                            <span id="carousel-counter" style="font-size: 1em; color: #333;">Image 1 / 4</span>
                            <button id="pause-btn" onclick="toggleAutoPlay()"
                                    style="background: #508af6; color: white; border: none;
                                           padding: 8px 16px; cursor: pointer; font-size: 0.9em; border-radius: 5px;">
                                ‚è∏ Pause
                            </button>
                        </div>
                    </div>

                    <figcaption style="margin-top: 20px;">
                        <strong>Qualitative comparison samples (500K + 200K steps, 512px resolution).</strong>
                        Models trained with End-to-End Tuned VAEs show improved quality across diverse prompts compared to FLUX-VAE.
                        Click images to zoom for detail inspection. Automatically cycles every 4 seconds.
                    </figcaption>
                </figure>
            </d-figure>

            <!-- Carousel JavaScript -->
            <script>
                let currentSlide = 0;
                const totalSlides = 4;
                let autoPlayInterval;
                let isPlaying = true;

                // Generate array of image paths
                const imagePaths = Array.from({length: totalSlides}, (_, i) => {
                    const num = String(i).padStart(2, '0');
                    return `static/img/qualitative/${num}.png`;
                });

                function showSlide(n) {
                    // Wrap around
                    if (n >= totalSlides) {
                        currentSlide = 0;
                    } else if (n < 0) {
                        currentSlide = totalSlides - 1;
                    } else {
                        currentSlide = n;
                    }

                    // Update image
                    const imgElement = document.getElementById('carousel-image');
                    imgElement.src = imagePaths[currentSlide];
                    imgElement.alt = `Sample ${currentSlide + 1}`;

                    // Update counter
                    document.getElementById('carousel-counter').textContent =
                        `Image ${currentSlide + 1} / ${totalSlides}`;
                }

                function changeSlide(direction) {
                    showSlide(currentSlide + direction);
                }

                function startAutoPlay() {
                    autoPlayInterval = setInterval(() => {
                        changeSlide(1);
                    }, 4000); // Change image every 4 seconds
                }

                function stopAutoPlay() {
                    clearInterval(autoPlayInterval);
                }

                function toggleAutoPlay() {
                    const btn = document.getElementById('pause-btn');
                    if (isPlaying) {
                        stopAutoPlay();
                        btn.innerHTML = '‚ñ∂ Play';
                        isPlaying = false;
                    } else {
                        startAutoPlay();
                        btn.innerHTML = '‚è∏ Pause';
                        isPlaying = true;
                    }
                }

                // Pause on hover
                const carouselContainer = document.querySelector('.carousel-container');
                carouselContainer.addEventListener('mouseenter', () => {
                    if (isPlaying) {
                        stopAutoPlay();
                    }
                });
                carouselContainer.addEventListener('mouseleave', () => {
                    if (isPlaying) {
                        startAutoPlay();
                    }
                });

                // Keyboard navigation
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowLeft') {
                        changeSlide(-1);
                    } else if (e.key === 'ArrowRight') {
                        changeSlide(1);
                    } else if (e.key === ' ') {
                        e.preventDefault();
                        toggleAutoPlay();
                    }
                });

                // Start autoplay on page load
                window.addEventListener('load', () => {
                    startAutoPlay();
                });
            </script>

            <style>
                .carousel-btn:hover {
                    background: rgba(0,0,0,0.8) !important;
                }

                #pause-btn:hover {
                    background: #3a6ed8 !important;
                }

                @media (max-width: 768px) {
                    .carousel-btn {
                        padding: 10px 15px !important;
                        font-size: 16px !important;
                    }
                }
            </style>
        </div>

        <hr>

        <!-- Latent Space Analysis Section -->
        <div id='latent-analysis' class="method-block">
            <h1 class="text">Latent Space Analysis</h1>

            <p class="text">
                To understand what makes End-to-End Tuned VAEs effective, we analyze the learned latent representations through PCA projections and spatial similarity analysis.
                These visualizations reveal how end-to-end tuning shapes the VAE's latent space to better support high-quality generation.
                Notably, end-to-end tuning enriches the VAE latent space by incorporating more structural and semantic information directly during the joint optimization process with the diffusion model.
            </p>

            <h3 class="text">PCA Projections of VAE Latents</h3>
            <p class="text">
                We project VAE latent representations to 2D using PCA and visualize them as RGB images (first 3 principal components).
                This reveals the spatial structure and semantic organization learned by different VAE architectures.
                As illustrated in the PCA visualizations, end-to-end training injects additional structural and semantic information into the latent representations.
            </p>

            <d-figure id="fig-pca-viz" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 10px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_0.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_3.png">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_6.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_7.png">
                        </div>
                    </div>
                    <figcaption>
                        <strong>PCA projection visualization.</strong>
                        Comparison of latent space structure between baseline FLUX-VAE and E2E-Tuned Flux-VAE.
                        Colors represent the first 3 principal components, revealing spatial and semantic organization.
                    </figcaption>
                </figure>
            </d-figure>

            <h3 class="text">Spatial Self-Similarity Analysis</h3>
            <p class="text">
                We compute cosine similarity between patch tokens in the latent space to measure spatial structure.
                End-to-End Tuned VAEs show more coherent spatial patterns, indicating better capture of local and global image structure.
                As shown in the similarity maps, end-to-end tuning embeds more meaningful structural and semantic relationships between patches, making the latent space more informative for the diffusion model to generate high-quality images.
            </p>

            <d-figure id="fig-similarity-viz" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 15px; margin-bottom: 20px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_0.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_2.png">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 15px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_4.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_6.png">
                        </div>
                    </div>
                    <figcaption>
                        <strong>Spatial self-similarity heatmaps.</strong>
                        Cosine similarity between latent patch tokens reveals spatial structure.
                        E2E-Tuned FLUX-VAE shows more coherent patterns indicating better structural representation.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="finding-box">
                <ul>
                    <li><strong>Key Finding:</strong> End-to-End tuned VAEs show improved semantic spatial structure and details over FLUX-VAE, as evidenced by PCA projections and spatial self-similarity analysis.</li>
                </ul>
            </div>
        </div>

        <hr>
        
        <!-- ImageNet Generalization Section -->
        <div id='imagenet' class="results-block">
            <h1 class="text">ImageNet Generalization</h1>

            <p class="text">
                Finally, we show that the end-to-end tuned VAEs can also be used for traditional image generation benchmarks like ImageNet<d-cite key="imagenet"></d-cite> 256x256.
                We observe that the end-to-end tuned VAEs achieve significantly better FID scores (better generation quality) compared to their original counterparts across all VAE families (FLUX<d-cite key="flux"></d-cite>, SD3.5<d-cite key="sd3.5"></d-cite>, Qwen<d-cite key="qwen-image"></d-cite>).
                <!-- A key question is whether VAEs optimized for T2I generation maintain their ability to encode natural images.
                We evaluate our REPA-E-T2I VAEs on ImageNet reconstruction tasks, measuring FID scores to assess encoding/decoding quality. -->
            </p>

            <d-figure id="fig-imagenet-convergence" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/e2e_convergence_fid.png" alt="ImageNet FID convergence" style="width: 100%;">
                    <figcaption>
                        <strong>ImageNet reconstruction quality (FID scores).</strong>
                        Comparison of Standard VAE vs E2E-tuned VAE across FLUX, Qwen, and SD3.5 architectures.
                        End-to-end tuned VAEs achieve significantly lower FID scores (6.1 vs 11.9 for FLUX, 5.2 vs 9.2 for Qwen, 5.2 vs 9.5 for SD3.5).
                    </figcaption>
                </figure>
            </d-figure>

            <div class="finding-box">
                <ul>
                    <li><strong>Key Finding:</strong> End-to-End VAEs improve generation performance while maintaining reconstruction fidelity across all VAE families (FLUX, SD3.5, Qwen-Image).</li>
                </ul>
            </div>

            <!-- <p class="text">
                <strong>Key findings:</strong>
                <ul class="text">
                    <li>E2E-tuned VAEs achieve <strong>substantially lower ImageNet FID scores</strong> across all architectures (roughly 45-50% improvement)</li>
                    <li>The improvements are consistent across different VAE architectures (FLUX, Qwen-Image, SD3.5)</li>
                    <li>End-to-end training enhances both T2I generation <em>and</em> natural image reconstruction capabilities</li>
                </ul>
            </p>

            <p class="text">
                This demonstrates that REPA-E-T2I VAEs serve as general-purpose image encoders, not just specialized T2I components.
                The joint optimization with diffusion transformers leads to representations that excel at both tasks.
            </p> -->
        </div>

        <!-- ImageNet Generalization Section
        <div id='imagenet' class="results-block">
            <h1 class="text">ImageNet Generalization</h1>

            <p class="text">
                A key question is whether VAEs optimized for T2I generation maintain their ability to encode natural images.
                We evaluate our REPA-E-T2I VAEs on ImageNet reconstruction tasks, measuring FID scores to assess encoding/decoding quality.
            </p>

            <d-figure id="fig-imagenet-convergence" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/e2e_convergence_fid.png" alt="ImageNet FID convergence" style="width: 100%;">
                    <figcaption>
                        <strong>ImageNet reconstruction quality (FID scores).</strong>
                        Comparison of Standard VAE vs E2E-tuned VAE across FLUX, Qwen, and SD3.5 architectures.
                        End-to-end tuned VAEs achieve significantly lower FID scores (6.1 vs 11.9 for FLUX, 5.2 vs 9.2 for Qwen, 5.2 vs 9.5 for SD3.5),
                        demonstrating improved reconstruction quality on ImageNet despite being optimized for T2I generation.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Key findings:</strong>
                <ul class="text">
                    <li>E2E-tuned VAEs achieve <strong>substantially lower ImageNet FID scores</strong> across all architectures (roughly 45-50% improvement)</li>
                    <li>The improvements are consistent across different VAE architectures (FLUX, Qwen-Image, SD3.5)</li>
                    <li>End-to-end training enhances both T2I generation <em>and</em> natural image reconstruction capabilities</li>
                </ul>
            </p>

            <p class="text">
                This demonstrates that REPA-E-T2I VAEs serve as general-purpose image encoders, not just specialized T2I components.
                The joint optimization with diffusion transformers leads to representations that excel at both tasks.
            </p>
        </div> -->

        <hr>

        <!-- Conclusion -->
        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Limitations and Future Work</h2>
            <p class="text">
                In this work, we show how end-to-end tuned VAEs lead to better T2I training over more standard VAEs like Flux-VAE and SDVAE which primarily trained for reconstruction alone.
                This primarily happens because end-to-end tuned VAEs lead to better semantic representations while maintaining strong reconstruction fidelity.
                <br><br>
                In future, we are actively studying the impact of end-to-end tuned VAEs for other downstream tasks which are also reliant on both the semantic and reconstrcution ability of the VAE latents. Examples include Image-to-Image Translation, Image-editing, Image inpainitng etc.
            </p>
        </div>

        <!-- Conclusion -->
        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                End-to-end VAEs show superior performance over their original counterparts across all benchmarks (COCO30k<d-cite key="coco30k"></d-cite>, DPG-Bench<d-cite key="dpgbench"></d-cite>, GenAI-Bench<d-cite key="genaibench"></d-cite>, GenEval<d-cite key="geneval"></d-cite>, MJHQ30k<d-cite key="mjhq30k"></d-cite>) without need for any additional representation alignment losses.
                <br><br>
                We hope REPA-E for T2I will inspire further research into end-to-end training strategies for generative models and the co-design of
                VAE architectures with diffusion transformers.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>Contributors</h3>
            <p style="font-size: 0.9em;">
                This work is a joint collaboration between the REPA-E team and Canva.
            </p>

            <p style="font-size: 0.9em; margin-top: 10px;">
                <strong>REPA-E Team:</strong>
                <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian Leng</a>,
                <a href="https://1jsingh.github.io/" target="_blank">Jaskirat Singh</a>,
                <a href="https://hou-yz.github.io/" target="_blank">Yunzhong Hou</a>,
                <a href="https://people.csiro.au/X/Z/Zhenchang-Xing/" target="_blank">Zhenchang Xing</a>,
                <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,
                <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang Zheng</a>
            </p>

            <p style="font-size: 0.9em; margin-top: 10px;">
                <strong>Canva Team:</strong>
                <!-- <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian Leng</a>, -->
                <a href="https://rynmurdock.github.io/" target="_blank">Ryan Murdock</a>,
                <a href="https://www.ethansmith2000.com/" target="_blank">Ethan Smith</a>,
                <a href="https://xiaoyang-rebecca.github.io/cv/" target="_blank">Rebecca Li</a>
                <!-- <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang Zheng</a> -->
            </p>

            <h3>BibTeX</h3>
            <div class="bibtex-container" style="position: relative;">
                <button class="copy-btn" id="copy-bibtex-btn" onclick="copyBibtex()" style="
                    position: absolute;
                    top: 10px;
                    right: 10px;
                    padding: 5px 10px;
                    background: #508af6;
                    color: white;
                    border: none;
                    border-radius: 4px;
                    cursor: pointer;
                    font-size: 0.85em;
                    opacity: 0;
                    transition: opacity 0.3s ease;
                    z-index: 10;
                ">üìã Copy</button>
                <p class="bibtex" id="bibtex-content">
                    @article{leng2025repae,<br>
                    &nbsp;&nbsp;title={{REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers}},<br>
                    &nbsp;&nbsp;author={Leng, Xingjian and Singh, Jaskirat and Hou, Yunzhong and Xing, Zhenchang and Xie, Saining and Zheng, Liang},<br>
                    &nbsp;&nbsp;journal={arXiv preprint arXiv:2504.10483},<br>
                    &nbsp;&nbsp;year={2025}<br>
                    }
                </p>
            </div>

            <!-- <h3>References</h3> -->
            <!-- Add BibTeX entries to references.bib file in the same directory -->
            <!-- Use <d-cite key="citation-key"></d-cite> in the text to cite papers -->
            <!-- Citations will appear as numbered references [1], [2], etc. -->
            <d-bibliography src="references.bib"></d-bibliography>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        <script src="./static/js/nav-bar.js"></script>

        <!-- Copy BibTeX functionality -->
        <style>
            .bibtex-container:hover .copy-btn {
                opacity: 1 !important;
            }

            .copy-btn:hover {
                background: #3a6ed8 !important;
                transform: translateY(-1px);
                box-shadow: 0 2px 4px rgba(0,0,0,0.2);
            }

            .copy-btn.copied {
                background: #4caf50 !important;
            }
        </style>

        <script>
            function copyBibtex() {
                const bibtexElement = document.getElementById('bibtex-content');
                const bibtexText = bibtexElement.innerText || bibtexElement.textContent;

                if (navigator.clipboard && navigator.clipboard.writeText) {
                    navigator.clipboard.writeText(bibtexText).then(() => {
                        showCopySuccess();
                    }).catch(err => {
                        copyWithFallback(bibtexText);
                    });
                } else {
                    copyWithFallback(bibtexText);
                }
            }

            function copyWithFallback(text) {
                const textarea = document.createElement('textarea');
                textarea.value = text;
                textarea.style.position = 'fixed';
                textarea.style.opacity = '0';
                document.body.appendChild(textarea);
                textarea.select();

                try {
                    document.execCommand('copy');
                    showCopySuccess();
                } catch (err) {
                    console.error('Failed to copy:', err);
                }

                document.body.removeChild(textarea);
            }

            function showCopySuccess() {
                const button = document.getElementById('copy-bibtex-btn');
                const originalText = button.innerHTML;

                button.innerHTML = '‚úì Copied!';
                button.classList.add('copied');

                setTimeout(() => {
                    button.innerHTML = originalText;
                    button.classList.remove('copied');
                }, 2000);
            }
        </script>
    </body>
</html>
