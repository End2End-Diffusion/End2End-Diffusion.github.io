<!doctype html>
<html lang="en">
    <head>
        <title>REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers</title>
        <link rel="icon" type="image/svg+xml" href="./static/img/icons/favicon.svg">
        <link rel="alternate icon" href="./static/img/icons/jellyfish.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://end2end-diffusion.github.io/repa-e-t2i" />
        <meta property="og:title" content="REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers" />
        <meta property="og:description" content="End-to-end tunable VAEs for improved text-to-image generation with diffusion transformers. Better performance across metrics with maintained ImageNet generalization." />

        <!-- Twitter -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="REPA-E for T2I: Family of end-to-end tuned VAEs for supercharging T2I diffusion transformers" />
        <meta name="twitter:description" content="End-to-end tunable VAEs for improved text-to-image generation with diffusion transformers." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script defer="" src="./static/js/hider.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/custom.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- medium zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">REPA-E <em>for</em> T2I</h1>
                    <h2>Family of end-to-end tuned VAEs for <em>supercharging</em><br>T2I diffusion transformers</h2>

                    <p>
                        We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                        <strong>End-to-end VAEs show superior performance over FLUX-VAE</strong> across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) without need for any additional representation alignment losses.
                    </p>

                    <!-- Key Points Icon Container -->
                    <div class="icon-container">
                        <div class="icon-item">
                            <img src="./static/img/icons/accuracy.svg" alt="Performance Icon">
                            <div><strong>Better T2I Performance</strong>: Consistent improvements over FLUX-VAE across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) with end-to-end tuned VAEs.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/spatial.svg" alt="Quality Icon">
                            <div><strong>Family of End-to-End Tuned VAEs</strong>: We release a family of end-to-end tuned VAEs across different families (Flux, SD3.5, Qwen-VAE) for supercharging text-to-image generation training.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/irepa.svg" alt="Generalization Icon">
                            <div><strong>Better Latent Space Structure</strong>: End-to-end tuned VAEs show better latent space structure compared to traditional VAEs (FLUX-VAE) which are mostly optimized for reconstruction.</div>
                        </div>
                    </div>

                    <div class="button-container">
                        <a href="https://arxiv.org/abs/2504.10483" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            REPA-E Paper
                        </a>
                        <a href="https://github.com/End2End-Diffusion/REPA-E" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="https://huggingface.co/REPA-E" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Models</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <!-- Placeholder for hero image -->
                    <img draggable="false" src="hero-v1.png" alt="REPA-E-T2I Sample" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <!-- <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://www.linkedin.com/in/xingjian-leng" class="author-link" target="_blank">Xingjian Leng<sup>1</sup></a> &emsp;
                    <a href="https://1jsingh.github.io/" class="author-link" target="_blank">Jaskirat Singh<sup>2,3</sup></a> &emsp;
                    <a href="#" class="author-link" target="_blank">Yunzhong Hou<sup>1</sup></a> &emsp;
                    <br>
                    <a href="#" class="author-link" target="_blank">Zhenchang Xing<sup>1</sup></a> &emsp;
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie<sup>4</sup></a> &emsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=vNHqr3oAAAAJ&view_op=list_works&sortby=pubdate" class="author-link" target="_blank">Liang Zheng<sup>1</sup></a>
                    <p></p>
                </p>
                <p style="text-align: center;">
                    <span class="affiliation-link"><sup>1</sup>Australian National University</span> &emsp;
                    <span class="affiliation-link"><sup>2</sup>Adobe Research</span> &emsp;
                    <span class="affiliation-link"><sup>3</sup>ANU</span> &emsp;
                    <span class="affiliation-link"><sup>4</sup>New York University</span>
                </p>
            </div>
        </div> -->

        <p class="text">
            We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
            End-to-end VAEs show superior performance across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) without need for any additional representation alignment losses.
        </p>

        <!-- Jump to Sections -->
        <div class="icon-row">
            <a href="#quantitative" class="icon-link">
                <img src="./static/img/icons/accuracy-blue.svg" alt="Quantitative" class="icon">
                Quantitative<br>Results (T2I)
            </a>
            <a href="#qualitative" class="icon-link">
                <img src="./static/img/icons/spatial-blue.svg" alt="Qualitative" class="icon">
                Qualitative<br>Comparison
            </a>
            <a href="#latent-analysis" class="icon-link">
                <img src="./static/img/icons/irepa-blue.svg" alt="Latent Analysis" class="icon">
                Latent Space<br>Analysis
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>

        <hr>

        <!-- Quantitative Results Section -->
        <div id='quantitative' class="motivation-block">
            <h1 class="text">Quantitative Results: T2I Performance</h1>

            <p class="text">
                We evaluate End-to-End Tuned VAEs across multiple benchmarks and training scenarios, demonstrating consistent improvements over baseline VAEs.
                Our end-to-end tuned VAEs show faster convergence and better final performance across all metrics.
            </p>

            <h3 class="text">Training Convergence at 100K Steps</h3>
            <p class="text">
                Starting with 100K training steps on COCO30k, we observe that End-to-End Tuned VAEs achieve better performance across all evaluation benchmarks.
                The improvements are particularly pronounced on vision-centric metrics like DPG-Bench and MJHQ30k.
            </p>

            <d-figure id="fig-convergence-100k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/convergence/3b-t2i-convergence-100k-combined.png" alt="Training convergence at 100K steps">
                    <figcaption>
                        <strong>Training convergence at 100K steps.</strong>
                        Comparison of FLUX-VAE baseline vs End-to-End Tuned VAEs across five benchmarks: COCO30k FID, DPG-Bench, GenAI-Bench, GenEval, and MJHQ30k.
                        End-to-end tuned VAEs show consistent improvements across all metrics.
                    </figcaption>
                </figure>
            </d-figure>

            <h3 class="text">Extended Training with Full Data (500K Steps)</h3>
            <p class="text">
                Extending training to 500K steps with the full dataset, End-to-End Tuned VAEs continue to outperform baseline VAEs.
                The performance gap remains consistent, demonstrating that the improvements are not limited to early training stages.
            </p>

            <d-figure id="fig-convergence-500k" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/convergence/3b-t2i-convergence-fulldata-500k-combined.png" alt="Training convergence at 500K steps with full data">
                    <figcaption>
                        <strong>Training convergence with full dataset (500K steps).</strong>
                        Extended training confirms sustained improvements across all benchmarks with End-to-End Tuned VAEs.
                    </figcaption>
                </figure>
            </d-figure>

            <h3 class="text">High-Resolution Training (512px, 200K Steps)</h3>
            <p class="text">
                When resuming training at higher resolution (512px) for 200K additional steps, End-to-End Tuned VAEs maintain their advantage.
                This demonstrates that the benefits of end-to-end tuning persist across different training regimes and resolutions.
            </p>

            <d-figure id="fig-convergence-res512" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/convergence/3b-t2i-convergence-fulldata-resume-res512-200k-combined.png" alt="Training convergence at 512px resolution">
                    <figcaption>
                        <strong>High-resolution training (512px, 200K steps).</strong>
                        Performance improvements persist when resuming training at higher resolution.
                    </figcaption>
                </figure>
            </d-figure>

        </div>

        <hr>

        <!-- Qualitative Comparison Section -->
        <div id='qualitative' class="spatial-structure-block">
            <h1 class="text">Qualitative Comparison</h1>

            <p class="text">
                Beyond quantitative metrics, End-to-End Tuned VAEs produces visually superior results compared to baseline FLUX-VAE.
                The generated images show improved detail, better prompt adherence, and more coherent compositions.
                Below we show comparisons for T2I generations for models training with FLUX-VAE and E2E-Tuned FLUX-VAE (Ours).
            </p>

            <!-- <h3 class="text">Sample Generations</h3>
            <p class="text">
                Below are selected examples from REPA-E-T2I generation.
                The carousel automatically cycles through all 30 samples. Use the controls to navigate or pause.
            </p> -->

            <d-figure id="fig-qualitative-carousel" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <!-- Carousel Container -->
                    <div class="carousel-container" style="position: relative; max-width: 100%; margin: 0 auto;">
                        <!-- Main Image Display -->
                        <div class="carousel-slides" style="position: relative; width: 100%; overflow: hidden;">
                            <img id="carousel-image" data-zoomable="" draggable="false"
                                 src="static/img/qualitative/00.png"
                                 alt="Sample 1"
                                 style="width: 100%; display: block; margin: 0 auto;">
                        </div>

                        <!-- Navigation Controls -->
                        <button class="carousel-btn carousel-prev" onclick="changeSlide(-1)"
                                style="position: absolute; left: 10px; top: 50%; transform: translateY(-50%);
                                       background: rgba(0,0,0,0.5); color: white; border: none;
                                       padding: 15px 20px; cursor: pointer; font-size: 20px; border-radius: 5px; z-index: 10;">
                            &#10094;
                        </button>
                        <button class="carousel-btn carousel-next" onclick="changeSlide(1)"
                                style="position: absolute; right: 10px; top: 50%; transform: translateY(-50%);
                                       background: rgba(0,0,0,0.5); color: white; border: none;
                                       padding: 15px 20px; cursor: pointer; font-size: 20px; border-radius: 5px; z-index: 10;">
                            &#10095;
                        </button>

                        <!-- Counter and Controls -->
                        <div class="carousel-info" style="text-align: center; margin-top: 15px; display: flex; justify-content: center; align-items: center; gap: 20px;">
                            <span id="carousel-counter" style="font-size: 1em; color: #333;">Image 1 / 24</span>
                            <button id="pause-btn" onclick="toggleAutoPlay()"
                                    style="background: #508af6; color: white; border: none;
                                           padding: 8px 16px; cursor: pointer; font-size: 0.9em; border-radius: 5px;">
                                ⏸ Pause
                            </button>
                        </div>
                    </div>

                    <figcaption style="margin-top: 20px;">
                        <strong>Qualitative comparison samples (500K steps, 512px resolution).</strong>
                        Models trained with End-to-End Tuned VAEs show improved quality across diverse prompts compared to FLUX-VAE.
                        Click images to zoom for detail inspection. Automatically cycles every 4 seconds.
                    </figcaption>
                </figure>
            </d-figure>

            <!-- Carousel JavaScript -->
            <script>
                let currentSlide = 0;
                const totalSlides = 24;
                let autoPlayInterval;
                let isPlaying = true;

                // Generate array of image paths
                const imagePaths = Array.from({length: totalSlides}, (_, i) => {
                    const num = String(i).padStart(2, '0');
                    return `static/img/qualitative/${num}.png`;
                });

                function showSlide(n) {
                    // Wrap around
                    if (n >= totalSlides) {
                        currentSlide = 0;
                    } else if (n < 0) {
                        currentSlide = totalSlides - 1;
                    } else {
                        currentSlide = n;
                    }

                    // Update image
                    const imgElement = document.getElementById('carousel-image');
                    imgElement.src = imagePaths[currentSlide];
                    imgElement.alt = `Sample ${currentSlide + 1}`;

                    // Update counter
                    document.getElementById('carousel-counter').textContent =
                        `Image ${currentSlide + 1} / ${totalSlides}`;
                }

                function changeSlide(direction) {
                    showSlide(currentSlide + direction);
                }

                function startAutoPlay() {
                    autoPlayInterval = setInterval(() => {
                        changeSlide(1);
                    }, 4000); // Change image every 4 seconds
                }

                function stopAutoPlay() {
                    clearInterval(autoPlayInterval);
                }

                function toggleAutoPlay() {
                    const btn = document.getElementById('pause-btn');
                    if (isPlaying) {
                        stopAutoPlay();
                        btn.innerHTML = '▶ Play';
                        isPlaying = false;
                    } else {
                        startAutoPlay();
                        btn.innerHTML = '⏸ Pause';
                        isPlaying = true;
                    }
                }

                // Pause on hover
                const carouselContainer = document.querySelector('.carousel-container');
                carouselContainer.addEventListener('mouseenter', () => {
                    if (isPlaying) {
                        stopAutoPlay();
                    }
                });
                carouselContainer.addEventListener('mouseleave', () => {
                    if (isPlaying) {
                        startAutoPlay();
                    }
                });

                // Keyboard navigation
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowLeft') {
                        changeSlide(-1);
                    } else if (e.key === 'ArrowRight') {
                        changeSlide(1);
                    } else if (e.key === ' ') {
                        e.preventDefault();
                        toggleAutoPlay();
                    }
                });

                // Start autoplay on page load
                window.addEventListener('load', () => {
                    startAutoPlay();
                });
            </script>

            <style>
                .carousel-btn:hover {
                    background: rgba(0,0,0,0.8) !important;
                }

                #pause-btn:hover {
                    background: #3a6ed8 !important;
                }

                @media (max-width: 768px) {
                    .carousel-btn {
                        padding: 10px 15px !important;
                        font-size: 16px !important;
                    }
                }
            </style>
        </div>

        <hr>

        <!-- Latent Space Analysis Section -->
        <div id='latent-analysis' class="method-block">
            <h1 class="text">Latent Space Analysis</h1>

            <p class="text">
                To understand what makes End-to-End Tuned VAEs effective, we analyze the learned latent representations through PCA projections and spatial similarity analysis.
                These visualizations reveal how end-to-end tuning shapes the VAE's latent space to better support high-quality generation.
            </p>

            <h3 class="text">PCA Projections of VAE Latents</h3>
            <p class="text">
                We project VAE latent representations to 2D using PCA and visualize them as RGB images (first 3 principal components).
                This reveals the spatial structure and semantic organization learned by different VAE architectures.
            </p>

            <d-figure id="fig-pca-viz" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 10px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_0.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_3.png">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_6.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/pca_7.png">
                        </div>
                    </div>
                    <figcaption>
                        <strong>PCA projection visualization.</strong>
                        Comparison of latent space structure between baseline FLUX-VAE and E2E-Tuned Flux-VAE.
                        Colors represent the first 3 principal components, revealing spatial and semantic organization.
                    </figcaption>
                </figure>
            </d-figure>

            <h3 class="text">Spatial Self-Similarity Analysis</h3>
            <p class="text">
                We compute cosine similarity between patch tokens in the latent space to measure spatial structure.
                End-to-End Tuned VAEs show more coherent spatial patterns, indicating better capture of local and global image structure.
            </p>

            <d-figure id="fig-similarity-viz" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 15px; margin-bottom: 20px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_0.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_2.png">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-bottom: 15px;">
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_4.png">
                        </div>
                        <div>
                            <img data-zoomable="" draggable="false" src="static/img/latent_viz/sim_6.png">
                        </div>
                    </div>
                    <figcaption>
                        <strong>Spatial self-similarity heatmaps.</strong>
                        Cosine similarity between latent patch tokens reveals spatial structure.
                        E2E-Tuned FLUX-VAE shows more coherent patterns indicating better structural representation.
                    </figcaption>
                </figure>
            </d-figure>
        </div>

        <hr>
        
        <!-- ImageNet Generalization Section -->
        <div id='imagenet' class="results-block">
            <h1 class="text">ImageNet Generalization</h1>

            <p class="text">
                Finally, we show that the end-to-end tuned VAEs can also be used for traditional image generation benchmarks like ImageNet 256x256.
                We observe that the end-to-end tuned VAEs achieve significantly better FID scores (better generation quality) compared to the traditional VAEs (FLUX-VAE), across all VAE families (Flux, Qwen, SD3.5).
                <!-- A key question is whether VAEs optimized for T2I generation maintain their ability to encode natural images.
                We evaluate our REPA-E-T2I VAEs on ImageNet reconstruction tasks, measuring FID scores to assess encoding/decoding quality. -->
            </p>

            <d-figure id="fig-imagenet-convergence" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/e2e_vae_convergence.png" alt="ImageNet FID convergence" style="width: 100%;">
                    <figcaption>
                        <strong>ImageNet reconstruction quality (FID scores).</strong>
                        Comparison of Standard VAE vs E2E-tuned VAE across FLUX, Qwen, and SD3.5 architectures.
                        End-to-end tuned VAEs achieve significantly lower FID scores (6.1 vs 11.9 for FLUX, 5.2 vs 9.2 for Qwen, 5.2 vs 9.5 for SD3.5).
                    </figcaption>
                </figure>
            </d-figure>

            <!-- <p class="text">
                <strong>Key findings:</strong>
                <ul class="text">
                    <li>E2E-tuned VAEs achieve <strong>substantially lower ImageNet FID scores</strong> across all architectures (roughly 45-50% improvement)</li>
                    <li>The improvements are consistent across different VAE architectures (FLUX, Qwen-Image, SD3.5)</li>
                    <li>End-to-end training enhances both T2I generation <em>and</em> natural image reconstruction capabilities</li>
                </ul>
            </p>

            <p class="text">
                This demonstrates that REPA-E-T2I VAEs serve as general-purpose image encoders, not just specialized T2I components.
                The joint optimization with diffusion transformers leads to representations that excel at both tasks.
            </p> -->
        </div>

        <!-- ImageNet Generalization Section
        <div id='imagenet' class="results-block">
            <h1 class="text">ImageNet Generalization</h1>

            <p class="text">
                A key question is whether VAEs optimized for T2I generation maintain their ability to encode natural images.
                We evaluate our REPA-E-T2I VAEs on ImageNet reconstruction tasks, measuring FID scores to assess encoding/decoding quality.
            </p>

            <d-figure id="fig-imagenet-convergence" style="margin-top: 30px; margin-bottom: 30px;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/e2e_vae_convergence.png" alt="ImageNet FID convergence" style="width: 100%;">
                    <figcaption>
                        <strong>ImageNet reconstruction quality (FID scores).</strong>
                        Comparison of Standard VAE vs E2E-tuned VAE across FLUX, Qwen, and SD3.5 architectures.
                        End-to-end tuned VAEs achieve significantly lower FID scores (6.1 vs 11.9 for FLUX, 5.2 vs 9.2 for Qwen, 5.2 vs 9.5 for SD3.5),
                        demonstrating improved reconstruction quality on ImageNet despite being optimized for T2I generation.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Key findings:</strong>
                <ul class="text">
                    <li>E2E-tuned VAEs achieve <strong>substantially lower ImageNet FID scores</strong> across all architectures (roughly 45-50% improvement)</li>
                    <li>The improvements are consistent across different VAE architectures (FLUX, Qwen-Image, SD3.5)</li>
                    <li>End-to-end training enhances both T2I generation <em>and</em> natural image reconstruction capabilities</li>
                </ul>
            </p>

            <p class="text">
                This demonstrates that REPA-E-T2I VAEs serve as general-purpose image encoders, not just specialized T2I components.
                The joint optimization with diffusion transformers leads to representations that excel at both tasks.
            </p>
        </div> -->

        <hr>

        <!-- Conclusion -->
        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We present REPA-E for T2I, a family of End-to-End Tuned VAEs for supercharging text-to-image generation training.
                End-to-end VAEs show superior performance over FLUX-VAE across all benchmarks (COCO30k, DPG-Bench, GenAI-Bench, GenEval, MJHQ30k) without need for any additional representation alignment losses.
                <br><br>
                We hope REPA-E for T2I will inspire further research into end-to-end training strategies for generative models and the co-design of
                VAE architectures with diffusion transformers.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <div class="bibtex-container" style="position: relative;">
                <button class="copy-btn" id="copy-bibtex-btn" onclick="copyBibtex()" style="
                    position: absolute;
                    top: 10px;
                    right: 10px;
                    padding: 5px 10px;
                    background: #508af6;
                    color: white;
                    border: none;
                    border-radius: 4px;
                    cursor: pointer;
                    font-size: 0.85em;
                    opacity: 0;
                    transition: opacity 0.3s ease;
                    z-index: 10;
                ">📋 Copy</button>
                <p class="bibtex" id="bibtex-content">
                    @article{leng2025repae,<br>
                    &nbsp;&nbsp;title={{REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers}},<br>
                    &nbsp;&nbsp;author={Leng, Xingjian and Singh, Jaskirat and Hou, Yunzhong and Xing, Zhenchang and Xie, Saining and Zheng, Liang},<br>
                    &nbsp;&nbsp;journal={arXiv preprint arXiv:2504.10483},<br>
                    &nbsp;&nbsp;year={2025}<br>
                    }
                </p>
            </div>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        <script src="./static/js/nav-bar.js"></script>

        <!-- Copy BibTeX functionality -->
        <style>
            .bibtex-container:hover .copy-btn {
                opacity: 1 !important;
            }

            .copy-btn:hover {
                background: #3a6ed8 !important;
                transform: translateY(-1px);
                box-shadow: 0 2px 4px rgba(0,0,0,0.2);
            }

            .copy-btn.copied {
                background: #4caf50 !important;
            }
        </style>

        <script>
            function copyBibtex() {
                const bibtexElement = document.getElementById('bibtex-content');
                const bibtexText = bibtexElement.innerText || bibtexElement.textContent;

                if (navigator.clipboard && navigator.clipboard.writeText) {
                    navigator.clipboard.writeText(bibtexText).then(() => {
                        showCopySuccess();
                    }).catch(err => {
                        copyWithFallback(bibtexText);
                    });
                } else {
                    copyWithFallback(bibtexText);
                }
            }

            function copyWithFallback(text) {
                const textarea = document.createElement('textarea');
                textarea.value = text;
                textarea.style.position = 'fixed';
                textarea.style.opacity = '0';
                document.body.appendChild(textarea);
                textarea.select();

                try {
                    document.execCommand('copy');
                    showCopySuccess();
                } catch (err) {
                    console.error('Failed to copy:', err);
                }

                document.body.removeChild(textarea);
            }

            function showCopySuccess() {
                const button = document.getElementById('copy-bibtex-btn');
                const originalText = button.innerHTML;

                button.innerHTML = '✓ Copied!';
                button.classList.add('copied');

                setTimeout(() => {
                    button.innerHTML = originalText;
                    button.classList.remove('copied');
                }, 2000);
            }
        </script>
    </body>
</html>
