<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #0E710E;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new {
    text-align: center;
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0;
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.green {
    color: #0E710E;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn:hover {
    color: #FF8563;
    transform: translateY(-2px);
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
    gap: 25px;
    font-size: 20px;
}

.github-btn:hover {
    color: #FF8563;
    transform: translateY(-2px);
}

.hf-btn:hover {
    color: #FF8563;
    transform: translateY(-2px);
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}

.image-grid {
    display: grid;
    grid-template-columns: 1fr 1.25fr;
    gap: 10px;
    width: 100%; 
}

.image-grid img {
    width: 100%; 
}

.image-grid img:first-child {
    object-fit: contain;
}
/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

.figure img {
    width: 100%; 
}

blockquote {
    background-color: rgba(40, 40, 40, 0.2);
    padding: 0px;
    margin: 2px 0;
    border-radius: 10px;
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 5px;
    padding-right: 5px;
}

/* Button styles */
.publication-links {
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 10px;
    margin: 15px 0;
}

.link-block {
    display: inline-block;
    margin: 0 5px;
}

.button.is-dark {
    background-color: #363636;
    color: white;
    padding: 10px 15px;
    font-size: 16px;
    display: inline-flex;
    align-items: center;
    transition: background-color 0.3s, transform 0.2s;
    text-decoration: none;
    border: none;
    cursor: pointer;
}

.button.is-dark:hover {
    background-color: #292929;
    transform: translateY(-2px);
}

.icon {
    margin-right: 8px;
    display: inline-flex;
    align-items: center;
    justify-content: center;
}

.button.is-rounded {
    background-color: #0E4E0E;
    color: white;
    padding: 10px 25px;
    font-size: 16px;
    display: inline-flex;
    align-items: center;
    transition: background-color 0.3s, transform 0.2s;
    text-decoration: none;
    border: none;
    cursor: pointer;
    border-radius: 25px;
}

.button.is-rounded:hover {
    background-color: #0A3D0A;
    transform: translateY(-2px);
}

</style>

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>REPA-E: Unlocking VAE for End-to-End Tuning of Latent Diffusion Transformers</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="REPA-E: Unlocking VAE for End-to-End Tuning of Latent Diffusion Transformers"/>
        <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
        <link href="https://fonts.googleapis.com/css2?family=FontAwesome" rel="stylesheet">
    </head>

 <body>
<div class="container">
    <div class="paper-title">
    <h1>
        <span class="green">REPA-E</span>: Unlocking VAE for End-to-End Tuning of Latent Diffusion Transformers
    </h1>
    </div>

    <div align="center">
        <a href="https://scholar.google.com.au/citations?user=GQzvqS4AAAAJ" target="_blank">Xingjian&nbsp;Leng</a><sup>1*</sup> &ensp; <b>&middot;</b> &ensp;
        <a href="https://1jsingh.github.io/" target="_blank">Jaskirat&nbsp;Singh</a><sup>1*</sup> &ensp; <b>&middot;</b> &ensp;
        <a href="https://hou-yz.github.io/" target="_blank">Yunzhong&nbsp;Hou</a><sup>1</sup> &ensp;
        <br>
        <a href="https://people.csiro.au/X/Z/Zhenchang-Xing/" target="_blank">Zhenchang&nbsp;Xing</a><sup>2</sup>&ensp; <b>&middot;</b> &ensp;
        <a href="https://www.sainingxie.com/" target="_blank">Saining&nbsp;Xie</a><sup>3</sup> &ensp; <b>&middot;</b> &ensp;
        <a href="https://zheng-lab-anu.github.io/" target="_blank">Liang&nbsp;Zheng</a><sup>1</sup> &ensp;
        <br>
        <sup>1</sup> Australian National University &emsp; <sup>2</sup>Data61-CSIRO &emsp; <sup>3</sup>New York University &emsp; <br>
        <sup>*</sup>Project Lead<br>
      </div>
    </center>

        <div style="clear: both">
            <div class="publication-links" style="margin-bottom: 30px;">
                <span class="link-block">
                    <a href="https://arxiv.org/abs/2504.10483" class="external-link button is-rounded" style="background-color: #0E4E0E; color: white;">
                        <span class="icon">
                            <i class="fas fa-file-alt"></i>
                        </span>
                        <span>Paper</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://github.com/End2End-Diffusion/REPA-E" class="external-link button is-rounded" style="background-color: #0E4E0E; color: white;">
                        <span class="icon">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://huggingface.co/REPA-E" class="external-link button is-rounded" style="background-color: #0E4E0E; color: white;">
                        <span class="icon" style="margin-right: 8px;">
                            🤗
                        </span>
                        <span>Model</span>
                    </a>
                </span>
            </div>
        </div>
        <center>
            <div class="figure">
                <img src="assets/viz-results-v4.webp" alt="REPA-E: Unlocking VAE for End-to-End Tuning of Latent Diffusion Transformers" style="width: 100%;" loading="lazy">
            </div>    
        </center>
    </div>
    <section id="news">
    <h2>News</h2>
    <hr>
    <div class="row">
        <div><span class="material-icons">event</span> [Oct 2025] 🚨 Released <a href="https://end2end-diffusion.github.io/repa-e-t2i/">REPA-E for T2I</a> 🚨 — a family of End-to-End Tuned VAEs:
            <ul style="margin-top: 8px; margin-bottom: 4px;">
                <li><b>Family of end-to-end tuned VAEs</b>:
                    <ul style="margin-top: 4px; margin-bottom: 4px;">
                        <li>T2I VAEs: <a href="https://huggingface.co/REPA-E/e2e-flux-vae">FLUX-VAE</a>, <a href="https://huggingface.co/REPA-E/e2e-sd3.5-vae">SD-3.5-VAE</a>, <a href="https://huggingface.co/REPA-E/e2e-qwenimage-vae">Qwen-Image-VAE</a></li>
                        <li>ImageNet VAEs: <a href="https://huggingface.co/REPA-E/e2e-sdvae-hf">SD-VAE</a>, <a href="https://huggingface.co/REPA-E/e2e-invae-hf">IN-VAE</a>, <a href="https://huggingface.co/REPA-E/e2e-vavae-hf">VA-VAE</a></li>
                    </ul>
                </li>
                <li><b>End-to-end training generalizes to T2I</b>: E2E-VAEs achieve better T2I generation quality across multiple resolutions (256×256, 512×512) compared to their standard VAE counterparts, without requiring additional representation alignment losses</li>
                <li><b>SOTA results on ImageNet 256×256</b>: FID <b>1.12</b> with CFG and <b>1.69</b> without CFG</li>
                <!-- <li><b>Improved latent space structure</b> with enhanced semantic spatial details compared to standard VAEs</li> -->
                <li>All models available as <b>Hugging Face-compatible AutoencoderKL</b> checkpoints — load directly with <code>diffusers</code> API, no custom wrapper needed</li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div><span class="material-icons">event</span> [Jun 2025] REPA-E accepted at ICCV 2025!</div>
    </div>
    <div class="row">
        <div><span class="material-icons">event</span> [Apr 2025] Paper, code, and pretrained models available on <a href="https://github.com/End2End-Diffusion/REPA-E">GitHub</a> and <a href="https://huggingface.co/REPA-E">Hugging Face</a>.</div>
    </div>
    </section>

    <section id="quickstart">
        <h2>Quickstart</h2>
        <hr>

        <style>
        .quickstart-box {
            background: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 30px;
            margin: 20px 0;
        }

        .tab-container {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
            border-bottom: 2px solid #ddd;
            padding-bottom: 10px;
        }

        .tab-button {
            padding: 10px 20px;
            background: #f0f0f0;
            border: none;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s;
            border-radius: 5px 5px 0 0;
        }

        .tab-button:hover {
            background: #e0e0e0;
        }

        .tab-button.active {
            background: #0E710E;
            color: white;
        }

        .tab-content {
            display: none;
            padding: 20px 0;
        }

        .tab-content.active {
            display: block;
        }

        .install-note {
            background: #ffffff;
            border-left: 4px solid #0E710E;
            padding: 15px;
            margin: 20px 0;
        }

        .install-note strong {
            font-weight: 600;
        }

        .quickstart-example {
            margin: 30px 0;
        }

        .quickstart-example h3 {
            color: #0E710E;
            margin-bottom: 10px;
        }

        .quickstart-example pre {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            overflow-x: auto;
        }
        </style>

        <div class="quickstart-box">
        <!-- Tab Buttons -->
        <div class="tab-container">
            <button class="tab-button active" onclick="switchTab('tab-flux')">E2E-FLUX-VAE</button>
            <button class="tab-button" onclick="switchTab('tab-sd35')">E2E-SD3.5-VAE</button>
            <button class="tab-button" onclick="switchTab('tab-qwen')">E2E-Qwen-Image-VAE</button>
            <button class="tab-button" onclick="switchTab('tab-sdvae')">E2E-SD-VAE</button>
            <button class="tab-button" onclick="switchTab('tab-vavae')">E2E-VA-VAE</button>
            <button class="tab-button" onclick="switchTab('tab-invae')">E2E-INVAE</button>
        </div>

        <!-- FLUX-VAE Tab -->
        <div id="tab-flux" class="tab-content active">
            <div class="install-note">
                <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
            </div>

            <div class="quickstart-example">
                <h3>Quick Start</h3>
                <p>Loading the VAE is as easy as:</p>
                <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-flux-vae").to("cuda")</code></pre>
            </div>

            <div class="quickstart-example">
                <h3>Complete Example</h3>
                <p>Full workflow for encoding and decoding images:</p>
                <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-flux-vae").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
            </div>
        </div>

        <!-- SD3.5-VAE Tab -->
        <div id="tab-sd35" class="tab-content">
            <div class="install-note">
                <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
            </div>

            <div class="quickstart-example">
                <h3>Quick Start</h3>
                <p>Loading the VAE is as easy as:</p>
                <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sd3.5-vae").to("cuda")</code></pre>
            </div>

            <div class="quickstart-example">
                <h3>Complete Example</h3>
                <p>Full workflow for encoding and decoding images:</p>
                <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sd3.5-vae").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
            </div>
        </div>

        <!-- Qwen-Image-VAE Tab -->
        <div id="tab-qwen" class="tab-content">
            <div class="install-note">
                <strong>Installation:</strong> <code>pip install diffusers>=0.35.0 torch>=2.5.0</code>
            </div>

            <div class="quickstart-example">
                <h3>Quick Start</h3>
                <p>Loading the VAE is as easy as:</p>
                <pre><code class="language-python">from diffusers import AutoencoderKLQwenImage

vae = AutoencoderKLQwenImage.from_pretrained("REPA-E/e2e-qwenimage-vae").to("cuda")</code></pre>
            </div>

            <div class="quickstart-example">
                <h3>Complete Example</h3>
                <p>Full workflow for encoding and decoding images (note the frame dimension handling):</p>
                <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKLQwenImage
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKLQwenImage.from_pretrained("REPA-E/e2e-qwenimage-vae").to(device)

# Add frame dimension (required for QwenImage VAE)
image_ = image.unsqueeze(2)

with torch.no_grad():
    latents = vae.encode(image_).latent_dist.sample()
    reconstructed = vae.decode(latents).sample

# Remove frame dimension
latents = latents.squeeze(2)
reconstructed = reconstructed.squeeze(2)</code></pre>
            </div>
        </div>

        <!-- SD-VAE Tab -->
        <div id="tab-sdvae" class="tab-content">
            <div class="install-note">
                <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
            </div>

            <div class="quickstart-example">
                <h3>Quick Start</h3>
                <p>Loading the VAE is as easy as:</p>
                <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sdvae-hf").to("cuda")</code></pre>
            </div>

            <div class="quickstart-example">
                <h3>Complete Example</h3>
                <p>Full workflow for encoding and decoding images (512×512 resolution):</p>
                <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content)).resize((512, 512))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-sdvae-hf").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
            </div>
        </div>

        <!-- VA-VAE Tab -->
        <div id="tab-vavae" class="tab-content">
            <div class="install-note">
                <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
            </div>

            <div class="quickstart-example">
                <h3>Quick Start</h3>
                <p>Loading the VAE is as easy as:</p>
                <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-vavae-hf").to("cuda")</code></pre>
            </div>

            <div class="quickstart-example">
                <h3>Complete Example</h3>
                <p>Full workflow for encoding and decoding images (512×512 resolution):</p>
                <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content)).resize((512, 512))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-vavae-hf").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
            </div>
        </div>

        <!-- InVAE Tab -->
        <div id="tab-invae" class="tab-content">
            <div class="install-note">
                <strong>Installation:</strong> <code>pip install diffusers>=0.33.0 torch>=2.3.1</code>
            </div>

            <div class="quickstart-example">
                <h3>Quick Start</h3>
                <p>Loading the VAE is as easy as:</p>
                <pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-invae-hf").to("cuda")</code></pre>
            </div>

            <div class="quickstart-example">
                <h3>Complete Example</h3>
                <p>Full workflow for encoding and decoding images (512×512 resolution):</p>
                <pre><code class="language-python">from io import BytesIO
import requests
from diffusers import AutoencoderKL
import numpy as np
import torch
from PIL import Image

response = requests.get("https://raw.githubusercontent.com/End2End-Diffusion/fuse-dit/main/assets/example.png")
device = "cuda"

image = torch.from_numpy(
    np.array(
        Image.open(BytesIO(response.content)).resize((512, 512))
    )
).permute(2, 0, 1).unsqueeze(0).to(torch.float32) / 127.5 - 1
image = image.to(device)

vae = AutoencoderKL.from_pretrained("REPA-E/e2e-invae-hf").to(device)

with torch.no_grad():
    latents = vae.encode(image).latent_dist.sample()
    reconstructed = vae.decode(latents).sample</code></pre>
            </div>
        </div>

        <script>
        function switchTab(tabId) {
            // Hide all tab contents
            var contents = document.getElementsByClassName('tab-content');
            for (var i = 0; i < contents.length; i++) {
                contents[i].classList.remove('active');
            }

            // Remove active class from all buttons
            var buttons = document.getElementsByClassName('tab-button');
            for (var i = 0; i < buttons.length; i++) {
                buttons[i].classList.remove('active');
            }

            // Show selected tab
            document.getElementById(tabId).classList.add('active');

            // Set active button
            event.target.classList.add('active');

            // Re-highlight code syntax
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        }
        </script>
        </div>

        <p style="margin-top: 0px;">For complete usage examples and integration with diffusion models, see the individual <a href="https://huggingface.co/REPA-E">model cards on Hugging Face</a>.</p>
    </section>

    <section>
        <h2>Overview</h2>
        <hr>
        <p>
            We address a fundamental question: <b><em style="color: green;">Can latent diffusion models and their VAE tokenizer be trained end-to-end?</em></b> While training both components jointly with standard diffusion loss is observed to be ineffective — often degrading final performance — we show that this limitation can be overcome using a simple representation-alignment (REPA) loss. Our proposed method, <b>REPA-E</b>, enables stable and effective joint training of both the VAE and the diffusion model, achieving state-of-the-art FID scores of <b>1.12</b> and <b>1.69</b> with and without classifier-free guidance on ImageNet 256×256.
        </p>

        <center>
            <div class="figure">
                <img src="assets/a-overview-v4.webp" alt="REPA-E Overview" style="max-width: 100%; height: auto;" loading="lazy">
            </div>
        </center>

        Through extensive evaluations, we demonstrate that our end-to-end training approach <b>REPA-E</b> offers four key advantages:
        <p>
            <b>1. Accelerated Generation Performance:</b> REPA-E significantly speeds up diffusion training by over <b>17×</b> and <b>45×</b> compared to REPA and vanilla training recipes, respectively, while achieving superior quality.
        </p>

        <p>
            <b>2. Improved VAE Latent-Space Structure:</b> Joint tuning adaptively enhances latent space structure across different VAE architectures, addressing their specific limitations without explicit regularization.
        </p>

        <p>
            <b>3. Superior Drop-in VAE Replacements:</b> The resulting <b>E2E-VAE</b> serves as a <b>drop-in replacement</b> for existing VAEs (e.g., SD-VAE), improving convergence and generation quality across diverse LDM architectures.
        </p>
        
        <p>
            <b>4. Effective From-Scratch Training:</b> REPA-E enables joint training of both VAE and LDM from scratch, still achieving superior performance compared to traditional training approaches.
        </p>
    </section>

    <section id="1-accelerated-performance">
        <h2>1. End-to-End Training Leads to Accelerated Generation Performance</h2>
        <hr>

        <ul>
            <li><b>Better performance with fewer epochs:</b> REPA-E achieves gFID of <b>4.07</b> in just 80 epochs, significantly outperforming MaskDiT (<b>5.69</b> with 1600 epochs) and FasterDiT (<b>7.91</b> with 400 epochs)</li>
            <li><b>Robust across architectures:</b> Performance improvements remain consistent across different model scales (SiT-B/L/XL) and VAE architectures (SD-VAE, IN-VAE, VA-VAE), demonstrating the method's versatility</li>
            <li><b>Enhanced image quality across training:</b> Using identical noise and labels, REPA-E generates structurally superior images compared to REPA baseline at 50K, 100K, and 400K training iterations</li>
        </ul>

        <center>
            <div class="figure">
                <img src="assets/repae-eval-combined.webp" alt="Combined" style="max-width: 95%; height: auto;" loading="lazy">
            </div>
        </center>

        <center>
            <div class="figure">
                <img src="assets/visual-scaling-v4.webp" alt="Qualitative comparison between REPA and REPA-E at different training iterations" style="max-width: 100%; height: auto;" loading="lazy">
            </div>
        </center>
    </section>

    <section id="2-improved-latent-space">
        <h2>2. End-to-End Training Improves VAE Latent-Space Structure</h2>
        <hr>
        <ul>
            <li><b>Adaptive refinement without explicit regularization:</b> REPA-E automatically adapts to each VAE's unique latent space characteristics without requiring manual heuristic-based regularization</li>
            <li><b>PCA visualization:</b> Using principal component analysis, we project VAE latents to RGB channels, revealing how end-to-end tuning with REPA-E improves latent representation quality</li>
            <li><b>Architecture-specific benefits:</b></li>
            <ul>
                <li><b>SD-VAE enhancement:</b> Reduces high-frequency noise components for smoother latent representations</li>
                <li><b>IN-VAE & VA-VAE enhancement:</b> Adds essential structural details to over-smoothed latent representations</li>
            </ul>
        </ul>

        <center>
            <div class="figure">
                <img src="assets/pca-analysis-v11.webp" alt="PCA Analysis of Latent Space" style="max-width: 90%; height: auto;" loading="lazy">
            </div>
        </center>
    </section>

    <section id="3-superior-drop-in-replacements">
        <h2>3. End-to-End Tuned VAEs as Superior Drop-in Replacements</h2>
        <hr>
        <ul>
            <li><b>Universal improvement:</b> E2E-VAE serves as a drop-in replacement for original VAEs, delivering superior performance across diverse diffusion architectures</li>
            <li><b>State-of-the-art generation quality:</b> Achieves gFID of <b>1.12</b> (w/ CFG) and <b>1.69</b> (w/o CFG) when training with REPA for 800 epochs</li>
            <li><b>Comprehensive performance superiority:</b> Achieves gFID of <b>3.46</b> with SiT-XL and REPA (vs. <b>4.88</b> with VA-VAE and <b>7.90</b> with SD-VAE) and <b>4.20</b> with DiT-XL (vs. <b>4.71</b> with VA-VAE and <b>12.29</b> with SD-VAE)</li>
            <li><b>Architecture-robust performance:</b> E2E-VAE maintains strong generation quality across diffusion models with and without REPA (e.g., gFID <b>3.46</b> on SiT-XL w/ REPA and <b>4.20</b> on DiT-XL w REPA)</li>
        </ul>

        <center>
            <div class="figure">
                <img src="assets/e2e-vae-eval.webp" alt="Drop-in VAE Performance Comparison" style="max-width: 100%; height: auto;" loading="lazy">
            </div>
            <p style="font-size: 0.8em; color: #555; margin-top: 8px;">
                Note: Results for the 800-epoch checkpoints of <b>REPA</b>, <b>LightningDiT</b>, and <b>E2E-VAE</b> in the left table are evaluated with 50 images per class (class-balanced sampling).
            </p>
        </center>
    </section>
    
    <section id="4-from-scratch-training">
        <h2>4. Enables Effective From-Scratch Training</h2>
        <hr>

        <div style="display: flex; align-items: top; gap: 40px;">
            <div style="flex: 1;">
                <ul>
                    <li><b>End-to-end training from scratch:</b> REPA-E can jointly train both VAE and LDM from scratch in an end-to-end manner, without requiring VAE pre-training</li>
                    <li><b>Strong performance even without initialization:</b> While initializing the VAE with pretrained weights helps slightly improve results, from-scratch training still achieves gFID of <b>4.34</b> at 80 epochs, significantly outperforming REPA (<b>7.90</b>)</li>
                </ul>
            </div>

            <div style="flex: 1; margin-top: 8px;">
                <div class="figure">
                    <img src="assets/repae-scratch-eval.webp" alt="From-Scratch Training Performance" style="width: 100%;" loading="lazy">
                </div>
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{leng2025repae,
  title={REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers},
  author={Xingjian Leng and Jaskirat Singh and Yunzhong Hou and Zhenchang Xing and Saining Xie and Liang Zheng},
  year={2025},
  journal={arXiv preprint arXiv:2504.10483},
}</code></pre>
    </section>

    <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd; font-size: 12px; color: #888; text-align: center;">
        <p>We thank the <a href="https://github.com/sihyun-yu/REPA/tree/gh-pages" target="_blank">REPA project</a> for the website template. Last updated: October 2025.</p>
    </footer>

</div>
</body>
</html>
